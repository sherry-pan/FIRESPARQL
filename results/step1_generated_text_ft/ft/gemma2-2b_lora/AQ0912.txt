Question:
What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?

Generated SPARQL:
SELECT DISTINCT ?metric ?metric_lbl
    WHERE {
      ?dataset       a                orkgc:Dataset;
                      rdfs:label       ?dataset_lbl.
      FILTER (str(?dataset_lbl) = "MLDoc Zero-Shot English-to-Italian")
      ?benchmark      orkgp:HAS_DATASET       ?dataset;
                      orkgp:HAS_EVALUATION    ?eval.
      ?paper         orkgp:HAS_BENCHMARK      ?benchmark;
                    rdfs:label               ?paper_lbl.
      OPTIONAL {?paper      orkgp:HAS_METRIC         ?metric;
                ?metric      rdfs:label               ?metric_lbl.}
      ?metric         rdfs:label               ?metric_lbl;
                      orkgp:HAS_VALUE           ?value.
      OPTIONAL {?value      xsd:float               ?value_num.}
    }
  }
<eos>
