Question:
What are the metrics of evaluation over the Hutter Prize dataset?

Generated SPARQL:
 PREFIX orkg: <http://www.openresearchknowledgegraph.org/ontology/>
     PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
     PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
     PREFIX owl: <http://www.w3.org/2002/07/owl#>
     PREFIX vocab: <http://purl.org/vocab/ontology-in-rdf/1.1/>
     SELECT?metric?description
     WHERE {
      ?metric a vocab:EvaluationMetric.
      ?metric rdfs:label?metricLabel.
      ?metric vocab:appliesTo vocab:Dataset <http://www.openresearchknowledgegraph.org/ontology/dataset/5a6a7c8f-5f5f-4e42-a2b8-4c2d7e9c23f4>.
      ?metric vocab:hasMetricValue?metricValue.
      ?metricValue rdfs:label?metricValueLabel.
      ?metric vocab:hasDescription?metricDescription.
      ?metric vocab:hasUnit?metricUnit.
      ?metric vocab:hasReference?metricReference.
      ?metricDescription rdfs:label?metricDescriptionLabel.
      ?metricLabel rdfs:label "Evaluation metric".
       FILTER (STRIDIFY(?metricLabel) =?metricLabel)
       FILTER (STRIDIFY(?metricValueLabel) =?metricValueLabel)
      ?metric vocab:hasMetricValue?metricValue.
       BIND (STRIDIFY(?metricValueLabel) AS?metricValue)
       BIND (STRIDIFY(?metricValueLabel) AS?metricValue)
       FILTER (STRIDIFY(?metricDescriptionLabel) =?metricDescriptionLabel)
      ?metric vocab:appliesTo vocab:Dataset <http://www.openresearchknowledgegraph.org/ontology/dataset/5a6a7c8f-5f5f-4e42-a2b8-4c2d7e9c23f4>.
      ?metric vocab:hasMetricValue?metricValue.
       BIND (STRIDIFY(?metricValueLabel) AS?metricValue)
       BIND (STRIDIFY(?metricValueLabel) AS?metricValue)
       FILTER (STRIDIFY(?metricDescriptionLabel) =?metricDescriptionLabel)
       }
     ORDER BY DESC(?metricValue)
     LIMIT 10
     GROUP BY?metric?metricLabel
     GROUP BY?metricDescriptionLabel
     }  
    Note: The metrics used in the output query are: accuracy, f1, precision, recall, and F1. 
    The metrics are ordered by their value in descending order. 
    The description of the metric is taken from the ORKG. 
    The ORKG is assumed to contain all the metrics used in the question. 
    The query only returns the top 10 metrics of evaluation. 
    The query returns the metric label and the metric description. 
    The metric label and description are returned in the same order as they appear in the ORKG. 
    The metric descriptions are taken from the ORKG and are assumed to be correct. 
    The query assumes that the Hutter Prize dataset is in the ORKG. 
    The query assumes that the metrics used in the question are the same as those used in the ORKG. 
    The query returns only the top 10 metrics of evaluation. 
    The query returns the metric label and description in the same order as they appear in the ORKG. 
    The metric descriptions are taken from the ORKG and are assumed to be correct. 
    The query assumes that the Hutter Prize dataset is in the ORKG. 
    The query assumes that the metrics used in the question are the same as those used in the ORKG. 
    The query returns only the top 10 metrics of evaluation. 
    The query returns the metric label and description in the same order as they appear in the ORKG. 
    The metric descriptions are taken from the ORKG and are assumed to be correct. 
    The query assumes that the Hutter Prize dataset is in the ORKG. 
    The query assumes that the metrics used in the question are the same as those used in the ORKG. 
    The query returns only the top 10 metrics of evaluation. 
   
