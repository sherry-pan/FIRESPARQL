Question:
What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?

Generated SPARQL:
 PREFIX orkg: <http://orkg.org/orkg/>
     SELECT?metric 
     WHERE {
    ?benchmark a orkg:Benchmark.
    ?benchmark orkg:hasBenchmark?benchmark_benchmark.
    ?benchmark_benchmark orkg:hasEvaluation?evaluation.
    ?evaluation orkg:hasMetric?metric.
     FILTER (CONTAINS(LCASE(?metric), "accuracy" ) OR CONTAINS(LCASE(?metric), "f1" ) OR CONTAINS(LCASE(?metric), "precision" ) OR CONTAINS(LCASE(?metric), "recall" ) OR CONTAINS(LCASE(?metric), "f1-score" ) OR CONTAINS(LCASE(?metric), "roc-auc" ) OR CONTAINS(LCASE(?metric), "ap" ) OR CONTAINS(LCASE(?metric), "auc" ) OR CONTAINS(LCASE(?metric), "fbeta" ) OR CONTAINS(LCASE(?metric), "mean-average-precision" ) OR CONTAINS(LCASE(?metric), "mean-precision" ) OR CONTAINS(LCASE(?metric), "mean-recall" ) OR CONTAINS(LCASE(?metric), "mean-recall-score" ) OR CONTAINS(LCASE(?metric), "mean-roc-auc" ) OR CONTAINS(LCASE(?metric), "roc-auc-score" ) OR CONTAINS(LCASE(?metric), "average-precision" ) OR CONTAINS(LCASE(?metric), "average-precision-score" ) OR CONTAINS(LCASE(?metric), "average-recall" ) OR CONTAINS(LCASE(?metric), "average-recall-score" ) OR CONTAINS(LCASE(?metric), "f1-score" ) OR CONTAINS(LCASE(?metric), "roc-auc-score" ) )
     }<|eot_id|>
