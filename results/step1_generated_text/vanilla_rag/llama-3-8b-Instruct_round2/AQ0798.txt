Question:
What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?

Generated SPARQL:
 PREFIX orkg: <http://orkg.org/orkg/>
     PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

     SELECT?metric
     WHERE {
      ?paper orkg:HAS_BENCHMARK?benchmark.
      ?benchmark orkg:HAS_EVALUATION?evaluation.
      ?evaluation orkg:HAS_METRIC?metric.
       FILTER (?metric rdfs:label?label.
              ?label = "F1-score" ||?label = "Accuracy" ||?label = "Precision" ||?label = "Recall" ||?label = "ROUGE-1" ||?label = "ROUGE-L" ||?label = "BLEU" ||?label = "METEOR")
     }<|eot_id|>
