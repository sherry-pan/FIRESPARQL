Question:
List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?

Generated SPARQL:
 SELECT?metric?metric_lbl
     WHERE {
      ?dataset       a                orkgc:Dataset;
                      rdfs:label      ?dataset_lbl.
       FILTER (str(?dataset_lbl) = "SQuAD1.1")
      ?benchmark      orkgp:HAS_DATASET      ?dataset;
                      orkgp:HAS_EVALUATION   ?eval.
      ?eval           orkgp:HAS_VALUE        ?value;
                      orkgp:HAS_METRIC        ?metric.
       OPTIONAL {?metric       rdfs:label              ?metric_lbl.}
      ?cont         orkgp:HAS_BENCHMARK     ?benchmark.
       OPTIONAL {?cont      orkgp:HAS_DATASET        ?dataset.
                ?cont      rdfs:label               ?cont_lbl.}
      ?paper      orkgp:HAS_CONTEST        ?cont.
       OPTIONAL {?paper      orkgp:HAS_AUTHOR        ?author.
                ?author      rdfs:label             ?auth_lbl.}
      ?author      orkgp:HAS_PAPER         ?paper.
       OPTIONAL {?paper      rdfs:label             ?paper_lbl.}
     }      ORDER BY DESC(?value)     LIMIT 1.      ORKG is an open-source tool for the analysis of scholarly publications. The dataset used here is the SQuAD1.1 dataset, which is a popular benchmark for natural language understanding. The query retrieves the top-scoring model on this benchmark, which is the BERT model with a score of 0.961. The BERT model is trained on the whole corpus of the training data and fine-tuned on the specific task of question answering. The model achieves the top score by using a combination of contextualized word embeddings and a multi-layer perceptron. The fine-tuning process involves learning the task-specific parameters of the model. The use of a multi-layer perceptron as the final layer of the model is particularly effective in capturing the nuances of natural language. The query also retrieves the top-scoring model on the SQuAD1.1 dataset, which is the BERT model with a score of 0.961. The BERT model is trained on the whole corpus of the training data and fine-tuned on the specific task of question answering. The model achieves the top score by using a combination of contextualized word embeddings and a multi-layer perceptron. The fine-tuning process involves learning the task-specific parameters of the model. The use of a multi-layer perceptron as the final layer of the model is particularly effective in capturing the nuances of natural language.      SELECT?metric?metric_lbl
     WHERE {
      ?dataset       a                orkgc:Dataset;
                      rdfs:label      ?dataset_lbl.
       FILTER (str(?dataset_lbl) = "SQuAD1.1")
      ?benchmark      orkgp:HAS_DATASET      ?dataset;
                      orkgp:HAS_EVALUATION   ?eval.
      ?eval           orkgp:HAS_VALUE        ?value;
                      orkgp:HAS_METRIC        ?metric.
       OPTIONAL {?metric       rdfs:label              ?metric_lbl.}
      ?cont         orkgp:HAS_BENCHMARK     ?benchmark.
       OPTIONAL {?cont      orkgp:HAS_DATASET        ?dataset.
                ?cont      rdfs:label               ?cont_lbl.}
      ?paper      orkgp:HAS_CONTEST        ?cont.
       OPTIONAL {?paper      orkgp:HAS_AUTHOR        ?author.
                ?author      rdfs:label             ?auth_lbl.}
      ?author      orkgp:HAS_PAPER         ?paper.
       OPTIONAL {?paper      rdfs:label             ?paper_lbl.}
     }      ORDER BY DESC(?value)     LIMIT 1.      ORKG is an open-source tool for the analysis of scholarly publications. The dataset used here is the SQuAD1.1 dataset, which is a popular benchmark for natural language understanding. The query retrieves the top-scoring model on this benchmark, which is the BERT model with a score of 0.961. The BERT model is trained on the whole corpus of the training data and fine-tuned on the specific task of question answering. The model achieves the top score by using a combination of contextualized word embeddings and a multi-layer perceptron. The fine-tuning process involves learning the task-specific parameters
