{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherrypan/miniconda3/envs/llama-factory/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.72s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, me hearty! Me name be Captain Chatbot, the scurviest pirate to ever sail the Seven Seas! Me be a swashbucklin' chatbot, here to regale ye with tales o' adventure, answer yer questions, and maybe even share a few sea shanties to keep ye entertained! So hoist the colors, me hearty, and let's set sail fer a chat!\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# 切换为你下载的模型文件目录, 这里的demo是Llama-3-8B-Instruct\n",
    "# 如果是其他模型，比如qwen，chatglm，请使用其对应的官方demo\n",
    "model_id = \"google-t5/t5-base\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.95s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本: \n",
      "The Open Research Knowledge Graph (ORKG) is a semantic knowledge graph designed to represent, compare, and retrieve scholarly contributions. Given a natural language question, your task is to generate the corresponding SPARQL query that can be used to query the ORKG for the correct answer. Give me only the SPARQL query, no other text.\n",
      "Input Question: Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\n",
      "Output SPARQL Query:\n",
      "\n",
      "SELECT?model?model_lbl\n",
      "WHERE {\n",
      " ?metric     a       orkgc:Metric;\n",
      "              rdfs:label ?metric_lbl.\n",
      "  FILTER (str(?metric_lbl) = \"Accuracy\")\n",
      "  {\n",
      "    SELECT?model?model_lbl\n",
      "    WHERE {\n",
      "     ?dataset       a                orkgc:Dataset;\n",
      "                      rdfs:label      ?dataset_lbl.\n",
      "      FILTER (str(?dataset_lbl) = \"Story Cloze Test\")\n",
      "     ?benchmark      orkgp:HAS_DATASET      ?dataset;\n",
      "                      orkgp:HAS_EVALUATION   ?eval.\n",
      "     ?eval           orkgp:HAS_VALUE        ?value;\n",
      "                      orkgp:HAS_METRIC        ?metric.\n",
      "     ?cont         orkgp:HAS_BENCHMARK     ?benchmark;\n",
      "                    orkgp:HAS_MODEL         ?model.\n",
      "     ?model      rdfs:label              ?model_lbl.\n",
      "    }\n",
      "    ORDER BY DESC(?value)\n",
      "    LIMIT 1\n",
      "  }\n",
      "} \n",
      "GROUP BY?model?model_lbl\n",
      "HAVING (MAX(?value) > 0) \n",
      "ORDER BY DESC(?value) DESC\n",
      "LIMIT 1\n",
      "``` \n",
      "This is the correct output SPARQL query. If you want to generate a query for a different question, just ask! \n",
      "\n",
      "Note: This query is quite complex and might not be suitable for beginners. It's recommended to practice with simpler queries before attempting more complex ones. \n",
      "\n",
      "Also, the output of this query might be different from the expected answer in the prompt, due to the way the query is written. This is because the query is designed to retrieve the model with the highest accuracy score, not the model with the highest score in the entire dataset, but rather the model with the highest score on the specific dataset mentioned in the question. \n",
      "\n",
      "Please let me know if you have any further questions or if there's anything else I can help you with. \n",
      "\n",
      "Let's generate the query for the next question. What is the number of papers that have been cited by the paper \"Title: A Deep Learning Approach to Named Entity Recognition\" with the highest citation count in the ORKG? \n",
      "Output SPARQL Query:\n",
      "``` \n",
      "SELECT (COUNT(?paper) AS?paper_count)\n",
      "WHERE {\n",
      " ?paper       orkgp:HAS_CITED_PAPER      ?cited_papers.\n",
      " ?cited_papers      rdfs:label              ?cited_papers_lbl.\n",
      "  FILTER (str(?cited_papers_lbl) = \"Papers with the following citations:\")\n",
      " ?cont         orkgp:HAS_PAPER          ?paper.\n",
      " ?paper      orkgp:HAS_CITED_PAPER      ?cited_papers.\n",
      " ?paper      rdfs:label              ?paper_lbl.\n",
      "  {\n",
      "    SELECT?paper?paper_lbl\n",
      "    WHERE {\n",
      "     ?dataset       a                orkgc:Dataset;\n",
      "                      rdfs:label      ?dataset_lbl.\n",
      "      FILTER (str(?dataset_lbl) = \"Title: A Deep Learning Approach to Named Entity Recognition\")\n",
      "     ?benchmark      orkgp:HAS_DATASET      ?dataset;\n",
      "                      orkgp:HAS_EVALUATION   ?eval.\n",
      "     ?eval           orkgp:HAS_VALUE        ?value;\n",
      "                      orkgp:HAS_METRIC        ?metric.\n",
      "     ?cont         orkgp:HAS_BENCHMARK     ?benchmark;\n",
      "                    orkgp:HAS_MODEL         ?model.\n",
      "     ?model      rdfs:label              ?model_lbl.\n",
      "      ORDER BY DESC(?value)\n",
      "      LIMIT 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "``` \n",
      "This is the correct output SPARQL query. If you want to generate a query for a different question, just ask! \n",
      "\n",
      "Note: As before, this query is quite complex and might not be suitable for beginners. It's recommended to practice with simpler queries before attempting more complex ones. \n",
      "\n",
      "Also, the output of this query might be different from the expected answer in the prompt, due to the way the query is written. This is because the query is designed to retrieve the paper with the highest citation count, not the paper with the highest citation count in the entire dataset, but rather the paper with the highest citation count mentioned in the question. \n",
      "\n",
      "Please let me know if you have any further questions or if there's anything else I can help you with. \n",
      "\n",
      "Let's generate the query for the next question. What is the average number of citations per paper for the dataset \"Title: A Deep Learning Approach to Named Entity Recognition\"? \n",
      "Output SPARQL Query\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 加载支持生成的模型\n",
    "local_model_path = \"llama3.2_3b_lora_merge\"  # 替换为适合的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "\n",
    "# 输入提示\n",
    "prompt = \"\"\"\n",
    "The Open Research Knowledge Graph (ORKG) is a semantic knowledge graph designed to represent, compare, and retrieve scholarly contributions. Given a natural language question, your task is to generate the corresponding SPARQL query that can be used to query the ORKG for the correct answer. Give me only the SPARQL query, no other text.\n",
    "Input Question: Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\n",
    "Output SPARQL Query:\n",
    "\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# 生成文本\n",
    "output_ids = model.generate(inputs[\"input_ids\"], max_length=1024, temperature=0.7)\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"生成的文本:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.04s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型生成的响应: Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset? as of my knowledge cutoff in March 2023?\n",
      "\n",
      "I don't have access to real-time data or updates after my knowledge cutoff in March 2023.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 本地模型路径\n",
    "local_model_path = \"llama3.2_3b_lora_merge\"\n",
    "\n",
    "# 加载本地分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "\n",
    "# 测试模型加载\n",
    "text = \"Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    max_length=50,  # Set a maximum length for the generated text\n",
    "    num_beams=5,    # Optional: Use beam search for more coherent responses\n",
    "    temperature=0.7, # Optional: Adjust temperature for diversity\n",
    ")\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"模型生成的响应:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read two csv files\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"results/step3_sparql_running_against_orkg/ft/llama3.2_3b_instruct_lora/retrieved_results.csv\")\n",
    "df2 = pd.read_csv(\"xueli_data/test_questions.csv\")\n",
    "\n",
    "# there are three columns in df2: id, question, and query,\n",
    "# and there are three columns in df1: question_id, status, message, gt_message\n",
    "# and the question_id in df1 is the same as the id in df2\n",
    "# so we need to merge the two dataframes on the id and question_id columns\n",
    "df = pd.merge(df1, df2, left_on=\"question_id\", right_on=\"id\")\n",
    "# and we need to keep only the columns: id, question, query, status, message, gt_message\n",
    "df = df[[\"id\", \"question\", \"query\", \"status\", \"message\", \"gt_message\"]]\n",
    "# filter the rows where the status is \"success\"\n",
    "df = df[df[\"status\"] == \"SUCCESS\"]\n",
    "# save the merged dataframe to a new csv file\n",
    "df.to_csv(\"results/step3_sparql_running_against_orkg/ft/llama3.2_3b_instruct_lora/merged_results.csv\", index=False)\n",
    "# ramdonly sample 30 rows from the merged dataframe\n",
    "df = df.sample(n=30, random_state=1)\n",
    "# save the sampled dataframe to a new csv file\n",
    "df.to_csv(\"results/step3_sparql_running_against_orkg/ft/llama3.2_3b_instruct_lora/30sample_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>query</th>\n",
       "      <th>status</th>\n",
       "      <th>message</th>\n",
       "      <th>gt_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AQ0495</td>\n",
       "      <td>List the title and ID of research papers that ...</td>\n",
       "      <td>SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AQ0278</td>\n",
       "      <td>Can you list the models that have been evaluat...</td>\n",
       "      <td>SELECT DISTINCT ?model ?model_lbl\\nWHERE {\\n  ...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>?model\\t?model_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "      <td>?model\\t?model_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AQ2309</td>\n",
       "      <td>List the code links in papers that use the A3C...</td>\n",
       "      <td>SELECT DISTINCT ?code\\nWHERE {\\n  ?model    a ...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>?code_link\\n\"https://github.com/4rChon/NL-FuN\"...</td>\n",
       "      <td>?code\\n\"https://github.com/4rChon/NL-FuN\"^^&lt;ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AQ0522</td>\n",
       "      <td>Provide a list of research paper titles and ID...</td>\n",
       "      <td>SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AQ0536</td>\n",
       "      <td>Provide a list of research paper titles and ID...</td>\n",
       "      <td>SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>AQ0507</td>\n",
       "      <td>What are the titles and IDs of research papers...</td>\n",
       "      <td>SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>AQ0498</td>\n",
       "      <td>Provide a list of research paper titles and ID...</td>\n",
       "      <td>SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>AQ0329</td>\n",
       "      <td>Can you list the models that have been evaluat...</td>\n",
       "      <td>SELECT DISTINCT ?model ?model_lbl\\nWHERE {\\n  ...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>?model\\t?model_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "      <td>?model\\t?model_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>AQ0301</td>\n",
       "      <td>What models are being evaluated on the WOS-119...</td>\n",
       "      <td>SELECT DISTINCT ?model ?model_lbl\\nWHERE {\\n  ...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>?model\\t?model_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "      <td>?model\\t?model_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>AQ0671</td>\n",
       "      <td>What are the titles and IDs of research papers...</td>\n",
       "      <td>SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "      <td>?paper\\t?paper_lbl\\n&lt;http://orkg.org/orkg/reso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           question  \\\n",
       "4    AQ0495  List the title and ID of research papers that ...   \n",
       "5    AQ0278  Can you list the models that have been evaluat...   \n",
       "6    AQ2309  List the code links in papers that use the A3C...   \n",
       "7    AQ0522  Provide a list of research paper titles and ID...   \n",
       "8    AQ0536  Provide a list of research paper titles and ID...   \n",
       "..      ...                                                ...   \n",
       "501  AQ0507  What are the titles and IDs of research papers...   \n",
       "502  AQ0498  Provide a list of research paper titles and ID...   \n",
       "506  AQ0329  Can you list the models that have been evaluat...   \n",
       "508  AQ0301  What models are being evaluated on the WOS-119...   \n",
       "512  AQ0671  What are the titles and IDs of research papers...   \n",
       "\n",
       "                                                 query   status  \\\n",
       "4    SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...  SUCCESS   \n",
       "5    SELECT DISTINCT ?model ?model_lbl\\nWHERE {\\n  ...  SUCCESS   \n",
       "6    SELECT DISTINCT ?code\\nWHERE {\\n  ?model    a ...  SUCCESS   \n",
       "7    SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...  SUCCESS   \n",
       "8    SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...  SUCCESS   \n",
       "..                                                 ...      ...   \n",
       "501  SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...  SUCCESS   \n",
       "502  SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...  SUCCESS   \n",
       "506  SELECT DISTINCT ?model ?model_lbl\\nWHERE {\\n  ...  SUCCESS   \n",
       "508  SELECT DISTINCT ?model ?model_lbl\\nWHERE {\\n  ...  SUCCESS   \n",
       "512  SELECT DISTINCT ?paper ?paper_lbl\\nWHERE {\\n  ...  SUCCESS   \n",
       "\n",
       "                                               message  \\\n",
       "4    ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...   \n",
       "5    ?model\\t?model_lbl\\n<http://orkg.org/orkg/reso...   \n",
       "6    ?code_link\\n\"https://github.com/4rChon/NL-FuN\"...   \n",
       "7    ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...   \n",
       "8    ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...   \n",
       "..                                                 ...   \n",
       "501  ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...   \n",
       "502  ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...   \n",
       "506  ?model\\t?model_lbl\\n<http://orkg.org/orkg/reso...   \n",
       "508  ?model\\t?model_lbl\\n<http://orkg.org/orkg/reso...   \n",
       "512  ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...   \n",
       "\n",
       "                                            gt_message  \n",
       "4    ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...  \n",
       "5    ?model\\t?model_lbl\\n<http://orkg.org/orkg/reso...  \n",
       "6    ?code\\n\"https://github.com/4rChon/NL-FuN\"^^<ht...  \n",
       "7    ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...  \n",
       "8    ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...  \n",
       "..                                                 ...  \n",
       "501  ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...  \n",
       "502  ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...  \n",
       "506  ?model\\t?model_lbl\\n<http://orkg.org/orkg/reso...  \n",
       "508  ?model\\t?model_lbl\\n<http://orkg.org/orkg/reso...  \n",
       "512  ?paper\\t?paper_lbl\\n<http://orkg.org/orkg/reso...  \n",
       "\n",
       "[205 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.31it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'base_model.model.model.model.embed_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m base_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     13\u001b[0m config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path,  \u001b[38;5;66;03m# e.g., \"meta-llama/llama-3-8b-Instruct\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path)\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-factory/lib/python3.10/site-packages/peft/peft_model.py:545\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    541\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](\n\u001b[1;32m    542\u001b[0m         model, config, adapter_name, autocast_adapter_dtype\u001b[38;5;241m=\u001b[39mautocast_adapter_dtype\n\u001b[1;32m    543\u001b[0m     )\n\u001b[0;32m--> 545\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-factory/lib/python3.10/site-packages/peft/peft_model.py:1151\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1147\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28mself\u001b[39m, max_memory\u001b[38;5;241m=\u001b[39mmax_memory, no_split_module_classes\u001b[38;5;241m=\u001b[39mno_split_module_classes\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[0;32m-> 1151\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_offload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapters_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m dispatch_model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffload_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offload_index\n\u001b[1;32m   1154\u001b[0m dispatch_model(\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1156\u001b[0m     device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   1157\u001b[0m     offload_dir\u001b[38;5;241m=\u001b[39moffload_dir,\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdispatch_model_kwargs,\n\u001b[1;32m   1159\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-factory/lib/python3.10/site-packages/peft/peft_model.py:1028\u001b[0m, in \u001b[0;36mPeftModel._update_offload\u001b[0;34m(self, offload_index, adapters_weights)\u001b[0m\n\u001b[1;32m   1026\u001b[0m suffix_pos \u001b[38;5;241m=\u001b[39m safe_key\u001b[38;5;241m.\u001b[39mrfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1027\u001b[0m extended_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m block_id \u001b[38;5;241m+\u001b[39m safe_key[:suffix_pos]\n\u001b[0;32m-> 1028\u001b[0m safe_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mextended_prefix\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(safe_module, BaseTunerLayer):\n\u001b[1;32m   1030\u001b[0m     final_key \u001b[38;5;241m=\u001b[39m extended_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.base_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m safe_key[suffix_pos:]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'base_model.model.model.model.embed_tokens'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
