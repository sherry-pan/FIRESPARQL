Question:
What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?

Generated SPARQL:
 PREFIX orkg: <http://www.openresearchknowledgegraph.org/ontology/>
     SELECT?metric 
     WHERE {
    ?metric rdfs:label "SST-5 Fine-grained classification dataset".
    ?metric rdfs:label?metricLabel.
    ?metric orkg:is_evaluated_by?benchmark.
    ?benchmark orkg:has_evaluation_metric?metric.
    ?benchmark orkg:has_dataset "SST-5 Fine-grained classification dataset".
     FILTER (REGEX(?metricLabel, "Accuracy|F1-score|Precision|Recall|F1|Macro F1|Area Under the Receiver Operating Characteristic Curve (AUC-ROC)|Area Under the Precision-Recall Curve (AUC-PR)|Mean Squared Error|Mean Absolute Error|Mean Absolute Percentage Error|Mean Squared Log Loss|Binary F1|Micro F1|Macro F1|F1 score|Precision|Recall|Area Under the Curve|F1|ROC-AUC|PR-AUC|Mean Squared Error|Mean Absolute Error|Mean Absolute Percentage Error|Mean Squared Log Loss|Binary F1|Macro F1|F1 score|Precision|Recall|Area Under the Curve|F1|ROC-AUC|PR-AUC|Mean Squared Error|Mean Absolute Error|Mean Absolute Percentage Error|Mean Squared Log Loss|Binary F1|Macro F1|F1 score|Precision|Recall|Area Under the Curve|F1|ROC-AUC|PR-AUC|Mean Squared Error|Mean Absolute Error|Mean Absolute Percentage Error|Mean Squared Log Loss|Binary F1|Macro F1|F1 score|Precision|Recall|Area Under the Curve|F1|ROC-AUC|PR-AUC
