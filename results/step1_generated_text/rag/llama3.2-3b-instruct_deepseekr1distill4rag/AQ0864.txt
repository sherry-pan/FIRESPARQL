Question:
What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?

Generated SPARQL:
 ```sparql
PREFIX orkg: <http://orkg.org/ontology/>
PREFIX orkp: <http://orkp.org/ontology/>
PREFIX orkc: <http://orkc.org/ontology/>
PREFIX orkgp: <http://orkgp.org/ontology/>

SELECT?metric?dataset?method
WHERE {
 ?metric orkg:relatedTo?dataset.
 ?metric orkp:hasEvaluationMethod?method.
  FILTER (str(?method) = "benchmarking models on")
 ?dataset orkg:relatedTo <http://orkg.org/dataset/Yelp_Fine_grained_classification_dataset>.
 ?metric orkg:label "Accuracy".
 ?metric orkg:label "Precision".
 ?metric orkg:label "Recall".
 ?metric orkg:label "F1 Score".
 ?metric orkg:label "ROC AUC".
}
```    Output ends here.<|eot_id|>
