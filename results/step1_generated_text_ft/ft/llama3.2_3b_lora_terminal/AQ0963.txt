Question:
What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?

Generated SPARQL:
 SELECT DISTINCT?metric
     WHERE {
      ?dataset       a                orkgc:Dataset;
                      rdfs:label      ?dataset_lbl.
       FILTER (str(?dataset_lbl) = "Atari 2600 Bowling")
      ?benchmark      orkgp:HAS_DATASET      ?dataset.
      ?cont           orkgp:HAS_BENCHMARK     ?benchmark.
      ?cont           a                    orkgp:Cont;
                      rdfs:label          ?cont_lbl.
      ?eval           orkgp:HAS_CONTRIBUTION   ?cont;
                      orkgp:P31               ?metric.
     }      ORDER BY DESC(?metric) LIMIT 10.     SELECT?metric
     WHERE {
      ?dataset       a                orkgc:Dataset;
                      rdfs:label      ?dataset_lbl.
       FILTER (str(?dataset_lbl) = "Atari 2600 Bowling")
      ?benchmark      orkgp:HAS_DATASET      ?dataset.
      ?cont           orkgp:HAS_BENCHMARK     ?benchmark.
      ?cont           a                    orkgp:Cont;
                      rdfs:label          ?cont_lbl.
      ?eval           orkgp:HAS_CONTRIBUTION   ?cont;
                      orkgp:P31               ?metric.
      ?metric         rdfs:label              ?metric_lbl.
       FILTER (str(?metric_lbl) = "Accuracy")
      ?cont           orkgp:HAS_EVALUATION    ?eval.
      ?eval           rdfs:label              ?eval_lbl.
       FILTER (str(?eval_lbl) = "Accuracy")
       FILTER (str(?metric_lbl) = "F1-score")
      ?cont           orkgp:HAS_EVALUATION    ?eval.
      ?eval           rdfs:label              ?eval_lbl.
       FILTER (str(?eval_lbl) = "F1-score")
      ?cont           orkgp:HAS_EVALUATION    ?eval.
      ?eval           rdfs:label              ?eval_lbl.
       FILTER (str(?eval_lbl) = "Mean Average Precision")
      ?cont           orkgp:HAS_EVALUATION    ?eval.
      ?eval           rdfs:label              ?eval_lbl.
       FILTER (str(?eval_lbl) = "Mean Average Precision")
      ?cont           orkgp:HAS_EVALUATION    ?eval.
      ?eval           rdfs:label              ?eval_lbl.
       FILTER (str(?eval_lbl) = "Mean squared error")
      ?cont           orkgp:HAS_EVALUATION    ?eval.
      ?eval           rdfs:label              ?eval_lbl.
       FILTER (str(?eval_lbl) = "Mean squared error")
      ?cont           orkgp:HAS_EVALUATION    ?eval.
      ?eval           rdfs:label              ?eval_lbl.
       FILTER (str(?eval_lbl) = "Number of frames")
      ?cont           orkgp:HAS_EVALUATION    ?eval.
      ?eval           rdfs:label              ?eval_lbl.
       FILTER (str(?eval_lbl) = "Number of frames")
     }      ORDER BY DESC(?metric) LIMIT 10.      SELECT?metric
     WHERE {
      ?dataset       a                orkgc:Dataset;
                      rdfs:label      ?dataset_lbl.
       FILTER (str(?dataset_lbl) = "Atari 2600 Bowling")
      ?benchmark      orkgp:HAS_DATASET      ?dataset.
      ?cont           orkgp:HAS_BENCHMARK     ?benchmark.
      ?cont           a                    orkgp:Cont;
                      rdfs:label          ?cont_lbl.
      ?eval           orkgp:HAS_CONTRIBUTION   ?cont;
                      orkgp:P31               ?metric.
      ?metric         rdfs:label              ?metric_lbl.
       FILTER (str(?metric_lbl) = "Accuracy")
      ?cont           orkgp:HAS_EVALUATION    ?eval.
      ?eval           rdfs:label              ?eval_lbl.
       FILTER (str(?eval_lbl) = "F1-score")
      ?cont           orkgp:HAS_EVALUATION    ?eval.
      ?eval           rdfs:label              ?eval_lbl.
       FILTER (str(?eval_lbl) = "Mean Average Precision")
      ?cont           orkgp:HAS_EVALUATION    ?eval.
      ?
