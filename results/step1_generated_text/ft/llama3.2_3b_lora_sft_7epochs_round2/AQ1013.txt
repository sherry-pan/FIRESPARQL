Question:
What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?

Generated SPARQL:
 SELECT?metric?metric_lbl
     WHERE {
      ?dataset       a                orkgc:Dataset;
                   rdfs:label      ?dataset_lbl.
       FILTER (str(?dataset_lbl) = "Sequential CIFAR-10")
      ?benchmark      orkgp:HAS_DATASET      ?dataset;
                      orkgp:HAS_EVALUATION   ?eval.
       OPTIONAL {?eval           orkgp:HAS_METRIC        ?metric.
               ?metric          rdfs:label              ?metric_lbl.}
       FILTER (str(?metric_lbl) = "accuracy")
       FILTER (str(?metric_lbl) = "loss")
       FILTER (str(?metric_lbl) = "F1-score")
       FILTER (str(?metric_lbl) = "ROC-AUC")
       FILTER (str(?metric_lbl) = "Precision")
       FILTER (str(?metric_lbl) = "Recall")
       FILTER (str(?metric_lbl) = "AP")
       FILTER (str(?metric_lbl) = "MSE")
       FILTER (str(?metric_lbl) = "Mean Squared Log Loss")
       FILTER (str(?metric_lbl) = "Mean Absolute Error")
       FILTER (str(?metric_lbl) = "Mean Squared Error")
       FILTER (str(?metric_lbl) = "Mean Absolute Error")
       FILTER (str(?metric_lbl) = "Mean Absolute Error")
       FILTER (str(?metric_lbl) = "MSE")
       FILTER (str(?metric_lbl) = "Mean Squared Error")
       FILTER (str(?metric_lbl) = "Mean Squared Error")
       FILTER (str(?metric_lbl) = "MAE")
       FILTER (str(?metric_lbl) = "Mean Absolute Error")
       FILTER (str(?metric_lbl) = "Mean Absolute Error")
       FILTER (str(?metric_lbl
