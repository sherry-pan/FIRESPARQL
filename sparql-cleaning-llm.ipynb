{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "# pip install nltk scikit-learn rouge-score\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from openai import OpenAI \n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating all generated text in a folder to get the sparql output, ignore the input natural language question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Get a list of all files in a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder.\n",
    "\n",
    "    Returns:\n",
    "        list: List of file names.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    try:\n",
    "        for entry in os.listdir(folder_path):\n",
    "            entry_path = os.path.join(folder_path, entry)\n",
    "            if os.path.isfile(entry_path):\n",
    "                files.append(entry)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The folder '{folder_path}' does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return files\n",
    "\n",
    "def get_completion(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"You are an expert in SPARQL query.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score between reference and hypothesis.\n",
    "    \"\"\"\n",
    "    smooth = SmoothingFunction().method1\n",
    "    # BLEU score with 4-gram smoothing: BLEU-4\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smooth, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L).\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load variables from .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key = api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate all files to extract the SPARQL queries, and save them in a new foldder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "folder_path = \"results/generated_text/llama3.2_3b_lora_merge\"\n",
    "file_names = get_files_in_folder(folder_path)\n",
    "\n",
    "failed_files = []\n",
    "sparql_folder = \"results/clean-sparql/llama3.2_3b_lora_merge\"\n",
    "os.makedirs(sparql_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "for file in file_names:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        question_sparql = f.read()\n",
    "        prompt = f\"Given a question and its corresponding SPARQL query, there might be some error in the SPARQL query such as missing blank space between variable names, unnecessary repetition and so on. Please clean the SPARQL and give me the most correct SPARQL that you think it's correct. Only output the SPARQL, no other text.\\n{question_sparql}\"\n",
    "        sparql_query= get_completion(prompt)\n",
    "        # print(f\"genterated sparql: {sparql_query}\")\n",
    "        new_file_path = os.path.join(sparql_folder, f\"{file}\")\n",
    "        with open(new_file_path, 'w') as new_file:\n",
    "            new_file.write(sparql_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU4 Score: 0.49\n",
      "Average ROUGE-1 Score: 0.76\n",
      "Average ROUGE-2 Score: 0.65\n",
      "Average ROUGE-L Score: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file with the test questions\n",
    "csv_file = 'xueli_data/test_questions.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "# print(df.head())\n",
    "\n",
    "bleu_score_list = []\n",
    "rouge_1_score_list = []\n",
    "rouge_2_score_list = []\n",
    "rouge_l_score_list = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "sparql_folder = \"results/clean-sparql/llama3.2_3b_lora_terminal\"\n",
    "sparql_list = get_files_in_folder(sparql_folder)\n",
    "# print(\"Files in folder:\", file_list[0:5])\n",
    "\n",
    "# Attension: not all the file in the test_questions.csv are in the clean_sparql folder\n",
    "for file in sparql_list:\n",
    "    # Load the content of the text file\n",
    "    with open(os.path.join(sparql_folder, file), 'r') as f:\n",
    "        generated_sparql = f.read().replace('```sparql', ' ').replace('```', ' ').strip()\n",
    "        # print(f'generated_sparql: {generated_sparql}')\n",
    "        question_id = file.split('.')[0]\n",
    "        # print(f'generated_sparql: {generated_sparql}')\n",
    "    # Get the ground truth SPARQL query\n",
    "    sparql = df[df['id'] == question_id]['query'].values[0]\n",
    "\n",
    "    # Calculate BLEU and ROUGE scores\n",
    "    bleu_score = calculate_bleu(sparql, generated_sparql)\n",
    "    rouge_scores = calculate_rouge(sparql, generated_sparql)\n",
    "    rouge_1 = rouge_scores['rouge-1']['f']\n",
    "    rouge_2 = rouge_scores['rouge-2']['f']\n",
    "    rouge_l = rouge_scores['rouge-l']['f']\n",
    "\n",
    "    # Append the scores to the lists\n",
    "    bleu_score_list.append(bleu_score)\n",
    "    rouge_1_score_list.append(rouge_1)\n",
    "    rouge_2_score_list.append(rouge_2)\n",
    "    rouge_l_score_list.append(rouge_l)\n",
    "\n",
    "# calculate the average scores\n",
    "avg_bleu_score = sum(bleu_score_list) / len(bleu_score_list)\n",
    "avg_rouge_1_score = sum(rouge_1_score_list) / len(rouge_1_score_list)\n",
    "avg_rouge_2_score = sum(rouge_2_score_list) / len(rouge_2_score_list)\n",
    "avg_rouge_l_score = sum(rouge_l_score_list) / len(rouge_l_score_list)\n",
    "print(f\"Average BLEU4 Score: {avg_bleu_score:.2f}\")\n",
    "print(f\"Average ROUGE-1 Score: {avg_rouge_1_score:.2f}\")\n",
    "print(f\"Average ROUGE-2 Score: {avg_rouge_2_score:.2f}\")\n",
    "print(f\"Average ROUGE-L Score: {avg_rouge_l_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
