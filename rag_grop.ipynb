{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import chromadb\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def rdf_data_indexing_to_chromadb(model, api_key, temperature,embed_model, rag_files, collection_name):\n",
    "    \"\"\"\n",
    "    Indexing the RDF data to ChromaDB\n",
    "    \"\"\"\n",
    "    llm = Groq(model= model, api_key=api_key)\n",
    "    # create client and a new collection\n",
    "    chroma_client = chromadb.EphemeralClient()\n",
    "    # clear past collection\n",
    "    try:\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "    except:\n",
    "        pass\n",
    "    # create new collection\n",
    "    chroma_collection = chroma_client.create_collection(collection_name)\n",
    "\n",
    "    # define embedding function\n",
    "    embed_model = HuggingFaceEmbedding(model_name=embed_model)\n",
    "\n",
    "    # load documents from a specific path(file or folders)\n",
    "    print(f\"Loading data from {rag_files}...\")\n",
    "    documents = SimpleDirectoryReader(rag_files).load_data()\n",
    "    print(f\"Data loaded successfully!\")\n",
    "    \n",
    "    # set up ChromaVectorStore and load in data\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    print(\"Indexing data...\")\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, storage_context=storage_context, embed_model=embed_model\n",
    "    )\n",
    "    print(\"Data indexed successfully!\")\n",
    "\n",
    "    Settings.llm = Groq(model = model, temperature=temperature)\n",
    "    query_engine = index.as_query_engine(llm)\n",
    "    print(\"Query Engine created successfully!\")\n",
    "    return query_engine\n",
    "\n",
    "\n",
    "def get_response_from_llm(query_engine, prompt):\n",
    "    \"\"\"\"\"\n",
    "    Get response from LLM\n",
    "    \"\"\"\n",
    "    response = query_engine.query(prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "def read_test_questions(question_file):\n",
    "    \"\"\"\n",
    "    Read 513 test questions from sciqa benchmark dataset\n",
    "    \"\"\"\n",
    "    # data = pd.read_csv(\"xueli_data/test_questions.csv\")\n",
    "    data = pd.read_csv(question_file)\n",
    "    questions = data['question'].tolist()\n",
    "    question_ids = data['id'].tolist()\n",
    "    return questions, question_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data indexed successfully!\n",
      "Query Engine created successfully!\n",
      "Time taken for indexing: 11085.139501094818 seconds\n",
      "RDF data indexed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# model = \"llama-3.3-70b-versatile\"\n",
    "# model = \"deepseek-r1-distill-llama-70b\"\n",
    "model = \"mixtral-8x7b-32768\"\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "temperature = 0.5\n",
    "embed_model = \"BAAI/bge-small-en\"\n",
    "rag_files = \"xueli_data/rdf-dump/\"\n",
    "collection_name = 'qa-kg'\n",
    "\n",
    "# rdf data indexing\n",
    "print(\"Indexing RDF data to ChromaDB...\")\n",
    "starttime = time.time()\n",
    "query_engine = rdf_data_indexing_to_chromadb(model, api_key, temperature,embed_model, rag_files, collection_name)\n",
    "endtime = time.time()\n",
    "print(f\"Time taken for indexing: {endtime - starttime} seconds\")\n",
    "print(\"RDF data indexed successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of questions: 513\n",
      "Processing the 511/513 question...............\n",
      "RAG Response for question AQ1169 is saved successfully!\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing the 512/513 question...............\n",
      "RAG Response for question AQ1864 is saved successfully!\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing the 513/513 question...............\n",
      "RAG Response for question AQ1087 is saved successfully!\n",
      "------------------------------------------------------------\n",
      "\n",
      "All questions are processed\n",
      "Time taken for processing all questions: 80.41597294807434 seconds\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "questions, question_ids = read_test_questions(\"xueli_data/test_questions.csv\")\n",
    "print(f\"Total number of questions: {len(questions)}\")\n",
    "# print(f\"The first question is: {questions[0]}\")\n",
    "# print(f\"The first question id is: {question_ids[0]}\")\n",
    "\n",
    "output_dir = f\"results/context_from_rag/prompt3_{model}\"\n",
    "# make a directory to save the results if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "prompt_template = \"\"\"Given a natural language question, your task is to retrieve the top 5 most similar candidate entities or properties from the provided ORKG RDF data dump in Turtle format. Follow these steps to complete the task:\n",
    "\n",
    "1. Extract Relevant Terms:\n",
    "- Identify key entities and properties explicitly or implicitly mentioned in the question.\n",
    "- Entities typically include datasets, models, or concepts (e.g., \"MPQA benchmark dataset\", \"models\").\n",
    "- Properties typically describe relationships or actions (e.g., \"evaluated on\").\n",
    "\n",
    "2. Find similar entities or properties:\n",
    "- Retrieve the top 10 most similar entities or properties from the RDF data dump.\n",
    "\n",
    "3. Select Top Candidates:\n",
    "- For each extracted entity or property, return the 5 most relevant matches.\n",
    "\n",
    "### Example Question:\n",
    "The input natural language question is: \"Could you provide a list of models that have been tested on the MPQA benchmark dataset?\"\n",
    "\n",
    "### Expected JSON Output Format\n",
    "Return a JSON object where results are grouped by the extracted terms, NO other text. The JSON structure should be:\n",
    "\n",
    "{\n",
    "\"question\": \"Could you provide a list of models that have been tested on the MPQA benchmark dataset?\",\n",
    "\"extracted_terms\": {\n",
    "    \"MPQA benchmark dataset\",\n",
    "    \"models\",\n",
    "    \"tested on\"\n",
    "},\n",
    "\"candidates\": {\n",
    "    \"MPQA benchmark dataset\": [\n",
    "    {\n",
    "        \"uri\": \"orkgr:R131168\",\n",
    "        \"rdfs:label\": \"Benchmark MPQA\",\n",
    "        \"rdf:type\": \"orkgc:Resource\"\n",
    "    },\n",
    "    {\n",
    "        \"uri\": \"orkgr:R131168\",\n",
    "        \"rdfs:label\": \"Benchmark MPQA\",\n",
    "        \"rdf:type\": \"orkgc:Benchmark\"\n",
    "    },\n",
    "    {\n",
    "        \"uri\": \"orkgr:R122689\",\n",
    "        \"rdfs:label\": \"MPQA\",\n",
    "        \"rdf:type\": \"orkgc:Resource\"\n",
    "    },\n",
    "    {\n",
    "        \"uri\": \"orkgr:R122689\",\n",
    "        \"rdfs:label\": \"MPQA\",\n",
    "        \"rdf:type\": \"orkgc:Dataset\"\n",
    "    },\n",
    "    {\n",
    "        \"uri\": \"orkgp:HAS_BENCHMARK\",\n",
    "    },\n",
    "    {\n",
    "        \"uri\": \"orkgp:HAS_DATASET\",\n",
    "    },\n",
    "    ]\n",
    "    \"models\": [\n",
    "    {\n",
    "        \"uri\": \"orkgp:HAS_MODEL\",\n",
    "    },\n",
    "    ],\n",
    "    \"evaluated on\": [\n",
    "    {\n",
    "        \"uri\": \"orkgp:HAS_EVALUATION\",\n",
    "    }\n",
    "    ],\n",
    "    \"tested on\": [\n",
    "    {\n",
    "        \"uri\": \"orkgp:HAS_EVALUATION\",\n",
    "    }\n",
    "    ]\n",
    "}\n",
    "}\n",
    "\n",
    "### Additional Refinements:\n",
    "- **Use RDF Prefixes for URIs**:\n",
    "- `\"orkgp:\"` for properties (predicates).\n",
    "- `\"orkgc:\"` for classes (concepts).\n",
    "- `\"orkgr:\"` for specific named resources (datasets, papers, models, etc.).\n",
    "- `\"rdf:\"` for general RDF terms.\n",
    "- `\"rdfs:\"` for schema-related terms.\n",
    "\n",
    "- **Include Entity Types (`rdf:type`)**:\n",
    "- `\"rdfs:Class\"` for general categories.\n",
    "- `\"orkgp:Predicate\"` for relationships/properties.\n",
    "- `\"orkgc:Resource\"` for dataset, models, or research objects.\n",
    "\n",
    "The input natural language question is:\n",
    "\"\"\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "for i in range(0, len(questions)):\n",
    "    print(f\"Processing the {i+1}/513 question...............\")\n",
    "    question = questions[i]\n",
    "    question_id = question_ids[i]\n",
    "    prompt = f\"{prompt_template}{question}\"\n",
    "    try:\n",
    "        response = get_response_from_llm(query_engine, prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        time.sleep(10)\n",
    "        response = get_response_from_llm(query_engine, prompt)\n",
    "    # save the response to a txt file in the output directory\n",
    "    with open(f\"{output_dir}/{question_id}.txt\", \"w\") as f:\n",
    "        f.write(str(response))\n",
    "    print(f\"RAG Response for question {question_id} is saved successfully!\")\n",
    "    print(\"------------------------------------------------------------\\n\")\n",
    "print(\"All questions are processed\")\n",
    "endtime = time.time()\n",
    "print(f\"Time taken for processing all questions: {endtime - starttime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
