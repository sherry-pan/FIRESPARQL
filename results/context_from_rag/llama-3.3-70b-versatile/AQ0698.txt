[
  {
    "property_iri": "http://orkg.org/orkg/predicate/P71079",
    "label": "Recall",
    "reason": "Recall is a common metric used to evaluate models, especially in information retrieval and classification tasks."
  },
  {
    "property_iri": "http://orkg.org/orkg/predicate/P71080",
    "label": "micro-F1",
    "reason": "Micro-F1 is a metric used to evaluate the performance of models, particularly in multi-class classification problems."
  },
  {
    "property_iri": "http://orkg.org/orkg/predicate/P71081",
    "label": "Accuracy",
    "reason": "Accuracy is a fundamental metric used to evaluate the performance of models, measuring the proportion of correctly classified instances."
  },
  {
    "property_iri": "http://orkg.org/orkg/predicate/P71082",
    "label": "macro-F1",
    "reason": "Macro-F1 is a metric used to evaluate the performance of models, particularly in multi-class classification problems, providing a more comprehensive picture than micro-F1."
  },
  {
    "property_iri": "http://orkg.org/orkg/predicate/P97107",
    "label": "third prize comment",
    "reason": "Although not a direct metric, this property might be related to evaluation or ranking of models, which could be relevant in the context of evaluating models on a dataset."
  },
  {
    "property_iri": "http://orkg.org/orkg/predicate/P71075",
    "label": "hasEvaluation",
    "reason": "This property indicates the presence of an evaluation, which could be related to the metrics used to evaluate models on the dataset."
  }
]