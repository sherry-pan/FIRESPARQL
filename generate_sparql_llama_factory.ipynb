{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "\n",
    "#%cd /content/LLaMA-Factory/\n",
    "\n",
    "args = dict(\n",
    "  model_name_or_path=\"llama3.2_3b_lora_merge\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
    "#   adapter_name_or_path=\"llama3_lora\",            # 加载之前保存的 LoRA 适配器\n",
    "  template=\"llama3\",                     # 和训练保持一致\n",
    "  # finetuning_type=\"lora\",                  # 和训练保持一致\n",
    "  # quantization_bit=4,                    # 加载 4 比特量化模型\n",
    ")\n",
    "chat_model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input question\n",
    "question = \"Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\"\n",
    "prompt = f\"\"\"\n",
    "The Open Research Knowledge Graph (ORKG) is a semantic knowledge graph designed to represent, compare, and retrieve scholarly contributions. Given a natural language question, your task is to generate the corresponding SPARQL query that can be used to query the ORKG for the correct answer. Give me only the SPARQL query, no other text.\n",
    "Input Question: {question}\n",
    "Only output the SPARQL query. \n",
    "No other text.\n",
    "\"\"\"\n",
    "\n",
    "# generate SPARQL query for the input question\n",
    "messages = []\n",
    "torch_gc()\n",
    "print(\"使用 `clear` 清除对话历史，使用 `exit` 退出程序。\")\n",
    "while True:\n",
    "  query = prompt\n",
    "  if query.strip() == \"exit\":\n",
    "    break\n",
    "  if query.strip() == \"clear\":\n",
    "    messages = []\n",
    "    torch_gc()\n",
    "    print(\"对话历史已清除\")\n",
    "    continue\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": query})\n",
    "  print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "  response = \"\"\n",
    "  for new_text in chat_model.stream_chat(messages):\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    response += new_text\n",
    "  print()\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "  print(\"\\n\", response)\n",
    "\n",
    "torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
