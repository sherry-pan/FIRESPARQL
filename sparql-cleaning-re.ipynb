{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "# pip install nltk scikit-learn rouge-score\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating all generated text in a folder to get the sparql output, ignore the input natural language question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Get a list of all files in a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder.\n",
    "\n",
    "    Returns:\n",
    "        list: List of file names.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    try:\n",
    "        for entry in os.listdir(folder_path):\n",
    "            entry_path = os.path.join(folder_path, entry)\n",
    "            if os.path.isfile(entry_path):\n",
    "                files.append(entry)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The folder '{folder_path}' does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return files\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score between reference and hypothesis.\n",
    "    \"\"\"\n",
    "    smooth = SmoothingFunction().method1\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smooth)\n",
    "\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L).\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate all files to extract the SPARQL queries, and save them in a new foldder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"results/generated_text/llama3.2_3b_instruct_lora\"\n",
    "file_names = get_files_in_folder(folder_path)\n",
    "\n",
    "failed_files = []\n",
    "sparql_folder = \"results/clean-sparql/llama3.2_3b_instruct_lora\"\n",
    "os.makedirs(sparql_folder, exist_ok=True)\n",
    "\n",
    "# Extract SPARQL queries from the generated text(including the input question and the output text)\n",
    "for file in file_names:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "        match = re.search(r\"Generated SPARQL:(.*?SELECT.*?)SELECT\", text, re.DOTALL)\n",
    "        if match:\n",
    "            extracted_text = match.group(1).strip()\n",
    "            # print(f\"File: {file}\\nExtracted Text:\\n{extracted_text}\\n\")\n",
    "            # save the extracted text to a new file with the name of the original file(question id)\n",
    "            new_file_path = os.path.join(sparql_folder, f\"{file}\")\n",
    "            with open(new_file_path, 'w') as new_file:\n",
    "                new_file.write(extracted_text)\n",
    "        else:\n",
    "            print(f\"No match found in file: {file}\\n\")\n",
    "            failed_files.append(file)\n",
    "\n",
    "print(f\"Failed to extract SPARQL queries from {len(failed_files)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file with the test questions\n",
    "csv_file = 'xueli_data/test_questions.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "# print(df.head())\n",
    "\n",
    "bleu_score_list = []\n",
    "rouge_1_score_list = []\n",
    "rouge_2_score_list = []\n",
    "rouge_l_score_list = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "sparql_folder = \"results/clean-sparql/llama3.2_3b_instruct_lora\"\n",
    "sparql_list = get_files_in_folder(sparql_folder)\n",
    "# print(\"Files in folder:\", file_list[0:5])\n",
    "\n",
    "# Attension: not all the file in the test_questions.csv are in the clean_sparql folder\n",
    "for file in sparql_list:\n",
    "    # Load the content of the text file\n",
    "    with open(os.path.join(sparql_folder, file), 'r') as f:\n",
    "        generated_sparql = f.read()\n",
    "        question_id = file.split('.')[0]\n",
    "        # print(f'generated_sparql: {generated_sparql}')\n",
    "    # Get the ground truth SPARQL query\n",
    "    sparql = df[df['id'] == question_id]['query'].values[0]\n",
    "\n",
    "    # Calculate BLEU and ROUGE scores\n",
    "    bleu_score = calculate_bleu(sparql, generated_sparql)\n",
    "    rouge_scores = calculate_rouge(sparql, generated_sparql)\n",
    "    rouge_1 = rouge_scores['rouge-1']['f']\n",
    "    rouge_2 = rouge_scores['rouge-2']['f']\n",
    "    rouge_l = rouge_scores['rouge-l']['f']\n",
    "\n",
    "    # Append the scores to the lists\n",
    "    bleu_score_list.append(bleu_score)\n",
    "    rouge_1_score_list.append(rouge_1)\n",
    "    rouge_2_score_list.append(rouge_2)\n",
    "    rouge_l_score_list.append(rouge_l)\n",
    "\n",
    "# calculate the average scores\n",
    "avg_bleu_score = sum(bleu_score_list) / len(bleu_score_list)\n",
    "avg_rouge_1_score = sum(rouge_1_score_list) / len(rouge_1_score_list)\n",
    "avg_rouge_2_score = sum(rouge_2_score_list) / len(rouge_2_score_list)\n",
    "avg_rouge_l_score = sum(rouge_l_score_list) / len(rouge_l_score_list)\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.2f}\")\n",
    "print(f\"Average ROUGE-1 Score: {avg_rouge_1_score:.2f}\")\n",
    "print(f\"Average ROUGE-2 Score: {avg_rouge_2_score:.2f}\")\n",
    "print(f\"Average ROUGE-L Score: {avg_rouge_l_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
