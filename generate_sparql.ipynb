{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 加载支持生成的模型\n",
    "local_model_path = \"llama3.2_3b_lora_merge\"  # 替换为适合的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherrypan/miniconda3/envs/llama-factory/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.99s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本: \n",
      "The Open Research Knowledge Graph (ORKG) is a semantic knowledge graph designed to represent, compare, and retrieve scholarly contributions. Given a natural language question, your task is to generate the corresponding SPARQL query that can be used to query the ORKG for the correct answer. Give me only the SPARQL query, no other text.\n",
      "Input Question: {question}\n",
      "Output SPARQL Query:\n",
      "\n",
      "SELECT DISTINCT?paper?paper_lbl (SUM(?paper?paper_rdf['orkgc:contributionTo']?cont) AS?cont_num)\n",
      "WHERE {\n",
      " ?cont?cont_lbl     a       orkgc:Contribution;\n",
      "              rdfs:label      ?cont_lbl.\n",
      " ?cont?cont_rdf      orkgp:HAS_CONTRIBUTION     ?cont.\n",
      "  OPTIONAL {?cont?cont_rdf      orkgp:P31           ?paper.}\n",
      "  OPTIONAL {?cont?cont_rdf      orkgp:HAS_PAPER     ?paper.}\n",
      "  OPTIONAL {?cont?cont_rdf      orkgp:P31           ?paper_lbl.}\n",
      "  OPTIONAL {?cont?cont_rdf      orkgp:HAS_PAPER_lbl ?paper_lbl.}\n",
      " ?paper      orkgp:P31           ?paper_lbl.\n",
      " ?paper      orkgp:HAS_PAPER     ?paper.\n",
      "} GROUP BY?paper?paper_lbl HAVING (SUM(?paper?paper_rdf['orkgc:contributionTo']?cont) > 1) ORDER BY DESC(?cont_num) LIMIT 10. \n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from pyorkg import orkgc, orkgp\n",
      "\n",
      "# Initialize the datasets\n",
      "dataset = orkgc.Dataset(\"http://example.org/ontology\")\n",
      "\n",
      "# Get the query\n",
      "query = \"\"\"\n",
      "SELECT DISTINCT?paper?paper_lbl (SUM(?paper?paper_rdf['orkgc:contributionTo']?cont) AS?cont_num)\n",
      "WHERE {\n",
      " ?cont?cont_lbl     a       orkgc:Contribution;\n",
      "              rdfs:label      ?cont_lbl.\n",
      " ?cont?cont_rdf      orkgp:HAS_CONTRIBUTION     ?cont.\n",
      "  OPTIONAL {?cont?cont_rdf      orkgp:P31           ?paper.}\n",
      "  OPTIONAL {?cont?cont_rdf      orkgp:HAS_PAPER     ?paper.}\n",
      "  OPTIONAL {?cont?cont_rdf      orkgp:P31           ?paper_lbl.}\n",
      "  OPTIONAL {?cont?cont_rdf      orkgp:HAS_PAPER_lbl ?paper_lbl.}\n",
      " ?paper      orkgp:P31           ?paper_lbl.\n",
      " ?paper      orkgp:HAS_PAPER     ?paper.\n",
      "} GROUP BY?paper?paper_lbl HAVING (SUM(?paper?paper_rdf['orkgc:contributionTo']?cont) > 1) ORDER BY DESC(?cont_num) LIMIT 10.\n",
      "\"\"\"\n",
      "\n",
      "# Execute the query\n",
      "results = dataset.query(query)\n",
      "\n",
      "# Convert the results to a pandas DataFrame\n",
      "df = pd.DataFrame(results)\n",
      "\n",
      "# Print the results\n",
      "print(df)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# input question\n",
    "question = \"Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\"\n",
    "\n",
    "# 输入提示\n",
    "prompt = \"\"\"\n",
    "The Open Research Knowledge Graph (ORKG) is a semantic knowledge graph designed to represent, compare, and retrieve scholarly contributions. Given a natural language question, your task is to generate the corresponding SPARQL query that can be used to query the ORKG for the correct answer. Give me only the SPARQL query, no other text.\n",
    "Input Question: {question}\n",
    "Output SPARQL Query:\n",
    "\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# 生成文本\n",
    "output_ids = model.generate(inputs[\"input_ids\"], max_length=1024, temperature=0.7)\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"生成的文本:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output sparql queries to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:677] 2024-12-13 13:46:17,580 >> loading configuration file llama3.2_3b_lora_merge/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-13 13:46:17,581 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"llama3.2_3b_lora_merge\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 13:46:17,582 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 13:46:17,583 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 13:46:17,583 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 13:46:17,583 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 13:46:17,584 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-12-13 13:46:17,867 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:677] 2024-12-13 13:46:17,884 >> loading configuration file llama3.2_3b_lora_merge/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-13 13:46:17,885 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"llama3.2_3b_lora_merge\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 13:46:17,902 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 13:46:17,902 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 13:46:17,903 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 13:46:17,903 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 13:46:17,903 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-12-13 13:46:18,198 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-13 13:46:18] llamafactory.data.template:157 >> Replace eos token: <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:677] 2024-12-13 13:46:18,279 >> loading configuration file llama3.2_3b_lora_merge/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-13 13:46:18,280 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"llama3.2_3b_lora_merge\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-13 13:46:18] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3934] 2024-12-13 13:46:18,282 >> loading weights file llama3.2_3b_lora_merge/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1670] 2024-12-13 13:46:18,282 >> Instantiating LlamaForCausalLM model under default dtype torch.float32.\n",
      "[INFO|configuration_utils.py:1096] 2024-12-13 13:46:18,283 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]\n",
      "[INFO|modeling_utils.py:4800] 2024-12-13 13:46:25,699 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2024-12-13 13:46:25,699 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at llama3.2_3b_lora_merge.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1049] 2024-12-13 13:46:25,712 >> loading configuration file llama3.2_3b_lora_merge/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2024-12-13 13:46:25,712 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-13 13:46:26] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2024-12-13 13:46:26] llamafactory.model.loader:157 >> all params: 3,212,749,824\n",
      "使用 `clear` 清除对话历史，使用 `exit` 退出程序。\n",
      "Assistant: SELECT DISTINCT?model?model_lbl\n",
      "WHERE {\n",
      " ?metric     a       orkgc:Metric;\n",
      "              rdfs:label ?metric_lbl.\n",
      "  FILTER (str(?metric_lbl) = \"Accuracy\")\n",
      "  {\n",
      "    SELECT?model?model_lbl\n",
      "    WHERE {\n",
      "     ?dataset       a                orkgc:Dataset;\n",
      "                      rdfs:label      ?dataset_lbl.\n",
      "      FILTER (str(?dataset_lbl) = \"Story Cloze Test\")\n",
      "     "
     ]
    }
   ],
   "source": [
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "\n",
    "#%cd /content/LLaMA-Factory/\n",
    "\n",
    "args = dict(\n",
    "  model_name_or_path=\"llama3.2_3b_lora_merge\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
    "#   adapter_name_or_path=\"llama3_lora\",            # 加载之前保存的 LoRA 适配器\n",
    "  template=\"llama3\",                     # 和训练保持一致\n",
    "  finetuning_type=\"lora\",                  # 和训练保持一致\n",
    "  # quantization_bit=4,                    # 加载 4 比特量化模型\n",
    ")\n",
    "chat_model = ChatModel(args)\n",
    "\n",
    "messages = []\n",
    "print(\"使用 `clear` 清除对话历史，使用 `exit` 退出程序。\")\n",
    "while True:\n",
    "  query = input(\"\\nUser: \")\n",
    "  if query.strip() == \"exit\":\n",
    "    break\n",
    "  if query.strip() == \"clear\":\n",
    "    messages = []\n",
    "    torch_gc()\n",
    "    print(\"对话历史已清除\")\n",
    "    continue\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": query})\n",
    "  print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "  response = \"\"\n",
    "  for new_text in chat_model.stream_chat(messages):\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    response += new_text\n",
    "  print()\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
