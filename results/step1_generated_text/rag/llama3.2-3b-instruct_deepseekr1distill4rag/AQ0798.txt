Question:
What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?

Generated SPARQL:
 ```
    PREFIX orkg: <http://example.org/orkg/>
    PREFIX orkgc: <http://example.org/orkgc/>
    PREFIX orkgp: <http://example.org/orkgp/>

    SELECT?metric?dataset?property
    WHERE {
     ?metric orkgc:resource?evaluationMetrics.
     ?evaluationMetrics orkgc:resource?dataset.
     ?dataset orkgc:resource?journalArticle.
     ?metric orkgp:property?benchmarkingModel.
     ?benchmarkingModel orkgp:property?property.
      FILTER (bc:label(?metric) = "evaluation metrics" @en)
      FILTER (bc:label(?dataset) = "MultiRC dataset" @en)
      FILTER (bc:label(?property) = "benchmarking models on" @en)
    }
    ```    Note: The `bc:label` function is used to get the English label of the resource, and the `FILTER` clause is used to filter the results to only include the correct resources. The `?metric`, `?dataset`, and `?property` variables are used to bind the results to the query variables.<|eot_id|>
