Question:
What are the metrics of evaluation over the Fashion-MNIST dataset?

Generated SPARQL:
 SELECT?metric,?metric_lbl
     WHERE {
       orkgp:R1M6G2H      orkgp:HAS_DATASET       orkgp:HAS_BENCHMARK      orkgp:HAS_METRIC.
      {
        orkgp:HAS_DATASET       orkgp:HAS_BENCHMARK      orkgp:HAS_METRIC.
        orkgp:HAS_DATASET       a                orkgp:Dataset.
        FILTER(REGEX(str(?dataset), "Fashion-MNIST"))
       ?dataset       a                orkgp:Dataset;
                       rdfs:label      ?dataset_lbl.
       ?benchmark      orkgp:HAS_DATASET      ?dataset;
                      orkgp:HAS_BENCHMARK     ?metric.
       ?metric         rdfs:label              ?metric_lbl.
      }
      UNION
      {
        orkgp:R1M6G2H      orkgp:HAS_DATASET       orkgp:HAS_METRIC.
        orkgp:HAS_DATASET       a                orkgp:Dataset.
        FILTER(REGEX(str(?dataset), "Fashion-MNIST"))
       ?dataset       a                orkgp:Dataset;
                       rdfs:label      ?dataset_lbl.
       ?metric         orkgp:HAS_DATASET      ?dataset;
                      orkgp:HAS_METRIC     ?cont.
       ?cont           orkgp:HAS_METRIC        ?metric.
       ?metric         rdfs:label              ?metric_lbl.
      }
    }
    Note: The provided SPARQL query is already optimized for performance. No further optimization is possible.
    Note: The provided SPARKG is already optimized for performance. No further optimization is possible. 

```python
def get_sparql_query():
    # Define the context
    context = {
        "extracted_terms": {
            "metrics of evaluation": ["Accuracy", "Precision", "Recall", "F1-score", "Area Under ROC Curve (AUC-ROC)"],
            "Fashion-MNIST dataset": ["
