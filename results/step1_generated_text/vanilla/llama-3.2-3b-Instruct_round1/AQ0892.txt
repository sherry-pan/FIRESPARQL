Question:
What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?

Generated SPARQL:
 PREFIX orkg: <http://www.openresearch.org/ontology/orkg/>
     PREFIX vocab: <http://www.openresearch.org/ontology/vocab/>
     SELECT?metric
     WHERE {
    ?metric vocab:relatedTo?dataset.
    ?dataset vocab:datasetName "Habitat 2020 Object Nav test-std".
    ?metric vocab:hasEvaluationMetric?metric.
     FILTER (STRLIKE(?metric, "accuracy" OR STRLIKE(?metric, "f1 score" OR STRLIKE(?metric, "mean squared error" OR STRLIKE(?metric, "mean absolute error")))
     ) 
     }<|eot_id|>
