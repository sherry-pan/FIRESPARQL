Question:
What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?

Generated SPARQL:
 PREFIX orkg: <http://example.org/orkg/>
     PREFIX orkg: <http://www.openresearchknowledgegraph.org/ontology/>

     SELECT?metric
     WHERE {
    ?metric orkg:evaluation?evaluation.
    ?metric orkg:hasBenchmark?benchmark.
    ?metric orkg:Recall?recall.
    ?metric orkg:hasF1?f1.
    ?metric orkg:Accuracy?accuracy.
    ?metric orkg:hasEvaluation?evaluation.
    ?metric orkg:NDCG@10?ndcg.
    ?metric orkg:QuantitativeMethods?quantitative.
    ?metric orkg:hasUncertaintyValue?uncertainty.
    ?metric orkg:hasUncertaintyUnit?unit.
    ?metric orkg:referenceSystem?system.
    ?metric orkg:classificationSystem?classification.
    ?metric orkg:reportingLevel?level.
    ?metric orkg:hasMethod?method.
    ?metric orkg:hasDataset?dataset.
    ?metric orkg:hasTrainingSet?training.
    ?metric orkg:hasValidationSet?validation.
    ?metric orkg:hasClasses?classes.
    ?metric orkg:numberOfType?type.
    ?metric orkg:propertyOfDataset?dataset.
    ?metric orkg:tabularDataset?tabular.
    ?metric orkg:datasetAnnotationType?annotation.
    ?metric orkg:splitInto?split.
    ?metric orkg:lessonLearned?learned.
    ?metric orkg:AlignedWithFAIRPrinciples?fair.
    ?metric orkg:numberOfParticipant?participant.
    ?metric orkg:numberOfTable?table.
    ?metric orkg:numberOfColumn?column.
    ?metric orkg:numberOfRow?row.
    ?metric orkg:numberOfClasses?classes.
    ?metric orkg:numberOfEntity?entity.
    ?metric orkg:averageRow?avgRow.
    ?metric orkg:averageColumn?avgColumn.
    ?metric orkg:CEATarget?cea.
    ?metric orkg:CTATarget?cta.
    ?metric orkg:CPATarget?cpa.
    ?metric orkg:acceptedSubmission?submission.
    ?metric orkg:rejectedSubmission?rejection.
    ?metric orkg:poster?poster.
    ?metric orkg:columnEntityAnnotation?entity.
    ?metric orkg:columnTypeAnnotation?type.
    ?metric orkg:columnPropertyAnnotation?property.
    ?metric orkg:matchingStrategy?strategy.
    ?metric orkg:systemModule?module.
    ?metric orkg:tableSelectionTechnique?technique.
    ?metric orkg:labelGenerationTechnique?generation.
    ?metric orkg:splittingTechnique?splitting.
    ?metric orkg:city?city.
    ?metric orkg:method?method.
    ?metric orkg:PM25pctChange?pm25.
    ?metric orkg:coLocatedWith?coLocated.
    ?metric orkg:DBpediaProperty?dbpedia.
    ?metric orkg:SchemaOrgProperty?schema.
    ?metric orkg:schemaOrgClass?schema.
    ?metric orkg:CTAParticipant?cta.
     }
     FILTER (str(?metric) = "evaluation" OR str(?metric) = "benchmark" OR str(?metric) = "Recall" OR str(?metric) = "micro-F1" OR str(?metric) = "macro-F1" OR str(?metric) = "Accuracy" OR str(?metric) = "hasEvaluation" OR str(?metric) = "NDCG@10" OR str(?metric) = "Quantitative methods" OR str(?metric) = "has uncertainty value" OR str(?metric) = "has uncertainty unit" OR str(?metric) = "reference system" OR str(?metric) = "classification system" OR str(?metric) = "reporting level" OR str(?metric) = "has method" OR str(?metric) = "method" OR str(?metric) = "has dataset" OR str(?metric) = "has training set" OR str(?metric) = "validation set" OR str(?metric) = "has classes" OR str(?metric) = "number of type" OR str(?metric) = "property of the dataset" OR str(?metric) = "tabular dataset" OR str(?metric) = "dataset annotation type" OR str(?metric) = "split into" OR str(?metric) = "lesson learned" OR str(?metric) = "Aligned with FAIR principles" OR str(?metric) = "number of participant" OR str(?metric) = "number of table" OR str(?metric) = "number of column" OR str(?metric) = "number of row
