Question:
What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?

Generated SPARQL:
 SELECT?metric?metric_lbl
     WHERE {
       orkgp:R1 orkgp:HAS_DATASET orkggp:R122689.
       orkgp:HAS_DATASET orkgp:HAS_EVALUATION_METRIC?metric.
      ?metric orkgp:RDEF?metric_lbl.
       FILTER (str(?metric_lbl) = "f1")
     }
     UNION
     SELECT?metric?metric_lbl
     WHERE {
       orkgp:R1 orkgp:HAS_DATASET orkggp:R122689.
       orkgp:HAS_DATASET orkgp:HAS_EVALUATION_METRIC?metric.
      ?metric orkgp:RDEF?metric_lbl.
       FILTER (str(?metric_lbl) = "accuracy")
     }
     UNION
     SELECT?metric?metric_lbl
     WHERE {
       orkgp:R1 orkgp:HAS_DATASET orkggp:R122689.
       orkgp:HAS_DATASET orkgp:HAS_EVALUATION_METRIC?metric.
      ?metric orkgp:RDEF?metric_lbl.
       FILTER (str(?metric_lbl) = "rouge-l")
     }    
    Note: The ORKG URIs are as follows:
    - orkgp:R1
    - orkgp:HAS_DATASET
    - orkggp:R122689
    - orkggp:HAS_EVALUATION_METRIC
    - orkggp:HAS_EVALUATION
    - orkggp:HAS_MODEL
    - orkggp:HAS_F1_SCORE
    - orkggp:HAS_ACCURACY
    - orkggp:HAS_RDEF
    - orkggp:HAS_RDEF
    - orkggp:RDEF
    The provided answer is based on the best of my knowledge and may not be accurate or up-to-date. Please verify it with the ORKG documentation and/or the actual data.<|eot_id|>
