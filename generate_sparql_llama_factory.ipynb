{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:677] 2024-12-13 14:39:35,600 >> loading configuration file llama3.2_3b_lora_merge/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-13 14:39:35,602 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"llama3.2_3b_lora_merge\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 14:39:35,612 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 14:39:35,612 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 14:39:35,613 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 14:39:35,613 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 14:39:35,613 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-12-13 14:39:36,133 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:677] 2024-12-13 14:39:36,135 >> loading configuration file llama3.2_3b_lora_merge/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-13 14:39:36,136 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"llama3.2_3b_lora_merge\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 14:39:36,137 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 14:39:36,137 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 14:39:36,137 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 14:39:36,138 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-13 14:39:36,138 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-12-13 14:39:36,438 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-13 14:39:36] llamafactory.data.template:157 >> Replace eos token: <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:677] 2024-12-13 14:39:36,452 >> loading configuration file llama3.2_3b_lora_merge/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-13 14:39:36,453 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"llama3.2_3b_lora_merge\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-13 14:39:36] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3934] 2024-12-13 14:39:36,456 >> loading weights file llama3.2_3b_lora_merge/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1670] 2024-12-13 14:39:36,467 >> Instantiating LlamaForCausalLM model under default dtype torch.float32.\n",
      "[INFO|configuration_utils.py:1096] 2024-12-13 14:39:36,473 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.40s/it]\n",
      "[INFO|modeling_utils.py:4800] 2024-12-13 14:39:51,331 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2024-12-13 14:39:51,332 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at llama3.2_3b_lora_merge.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1049] 2024-12-13 14:39:51,417 >> loading configuration file llama3.2_3b_lora_merge/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2024-12-13 14:39:51,418 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-13 14:39:51] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2024-12-13 14:39:51] llamafactory.model.loader:157 >> all params: 3,212,749,824\n"
     ]
    }
   ],
   "source": [
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "\n",
    "#%cd /content/LLaMA-Factory/\n",
    "\n",
    "args = dict(\n",
    "  model_name_or_path=\"llama3.2_3b_lora_merge\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
    "#   adapter_name_or_path=\"llama3_lora\",            # 加载之前保存的 LoRA 适配器\n",
    "  template=\"llama3\",                     # 和训练保持一致\n",
    "  # finetuning_type=\"lora\",                  # 和训练保持一致\n",
    "  # quantization_bit=4,                    # 加载 4 比特量化模型\n",
    ")\n",
    "chat_model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 `clear` 清除对话历史，使用 `exit` 退出程序。\n",
      "Assistant: SELECT DISTINCT?model?model_lbl\n"
     ]
    }
   ],
   "source": [
    "# input question\n",
    "question = \"Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\"\n",
    "prompt = f\"\"\"\n",
    "The Open Research Knowledge Graph (ORKG) is a semantic knowledge graph designed to represent, compare, and retrieve scholarly contributions. Given a natural language question, your task is to generate the corresponding SPARQL query that can be used to query the ORKG for the correct answer. Give me only the SPARQL query, no other text.\n",
    "Input Question: {question}\n",
    "Output SPARQL Query here:\n",
    "\"\"\"\n",
    "\n",
    "# generate SPARQL query for the input question\n",
    "messages = []\n",
    "torch_gc()\n",
    "print(\"使用 `clear` 清除对话历史，使用 `exit` 退出程序。\")\n",
    "while True:\n",
    "  query = prompt\n",
    "  if query.strip() == \"exit\":\n",
    "    break\n",
    "  if query.strip() == \"clear\":\n",
    "    messages = []\n",
    "    torch_gc()\n",
    "    print(\"对话历史已清除\")\n",
    "    continue\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": query})\n",
    "  print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "  response = \"\"\n",
    "  for new_text in chat_model.stream_chat(messages):\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    response += new_text\n",
    "  print()\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "  print(\"\\n\", response)\n",
    "\n",
    "torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
