{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import chromadb\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def rdf_data_indexing_to_chromadb(model, api_key, temperature,embed_model, rag_files, collection_name):\n",
    "    \"\"\"\n",
    "    Indexing the RDF data to ChromaDB\n",
    "    \"\"\"\n",
    "    llm = Groq(model= model, api_key=api_key)\n",
    "    # create client and a new collection\n",
    "    chroma_client = chromadb.EphemeralClient()\n",
    "    # clear past collection\n",
    "    try:\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "    except:\n",
    "        pass\n",
    "    # create new collection\n",
    "    chroma_collection = chroma_client.create_collection(collection_name)\n",
    "\n",
    "    # define embedding function\n",
    "    embed_model = HuggingFaceEmbedding(model_name=embed_model)\n",
    "\n",
    "    # load documents from a specific path(file or folders)\n",
    "    print(f\"Loading data from {rag_files}...\")\n",
    "    documents = SimpleDirectoryReader(rag_files).load_data()\n",
    "    print(f\"Data loaded successfully!\")\n",
    "    \n",
    "    # set up ChromaVectorStore and load in data\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    print(\"Indexing data...\")\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, storage_context=storage_context, embed_model=embed_model\n",
    "    )\n",
    "    print(\"Data indexed successfully!\")\n",
    "\n",
    "    Settings.llm = Groq(model = model, temperature=temperature)\n",
    "    query_engine = index.as_query_engine(llm)\n",
    "    print(\"Query Engine created successfully!\")\n",
    "    return query_engine\n",
    "\n",
    "\n",
    "def get_response_from_llm(query_engine, prompt):\n",
    "    \"\"\"\"\"\n",
    "    Get response from LLM\n",
    "    \"\"\"\n",
    "    response = query_engine.query(prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "def read_test_questions(question_file):\n",
    "    \"\"\"\n",
    "    Read 513 test questions from sciqa benchmark dataset\n",
    "    \"\"\"\n",
    "    # data = pd.read_csv(\"xueli_data/test_questions.csv\")\n",
    "    data = pd.read_csv(question_file)\n",
    "    questions = data['question'].tolist()\n",
    "    question_ids = data['id'].tolist()\n",
    "    return questions, question_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# model = \"llama-3.3-70b-versatile\"\n",
    "model = \"deepseek-r1-distill-llama-70b\"\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "temperature = 0.5\n",
    "embed_model = \"BAAI/bge-small-en\"\n",
    "rag_files = \"xueli_data/rdf-dump/\"\n",
    "collection_name = 'qa-kg'\n",
    "\n",
    "# rdf data indexing\n",
    "print(\"Indexing RDF data to ChromaDB...\")\n",
    "starttime = time.time()\n",
    "query_engine = rdf_data_indexing_to_chromadb(model, api_key, temperature,embed_model, rag_files, collection_name)\n",
    "endtime = time.time()\n",
    "print(f\"Time taken for indexing: {endtime - starttime} seconds\")\n",
    "print(\"RDF data indexed successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, question_ids = read_test_questions(\"xueli_data/test_questions.csv\")\n",
    "print(f\"Total number of questions: {len(questions)}\")\n",
    "# print(f\"The first question is: {questions[0]}\")\n",
    "# print(f\"The first question id is: {question_ids[0]}\")\n",
    "\n",
    "output_dir = \"results/rag_groq/prompt2\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "for i in range(0, len(questions)):\n",
    "    print(f\"Processing the {i}/513 question...............\")\n",
    "    question = questions[i]\n",
    "    question_id = question_ids[i]\n",
    "\n",
    "    prompt = \"\"\"Given a natural language question, your task is to retrieve the top 5 most similar candidate entities or properties from the provided ORKG RDF data dump in Turtle format. Follow these steps to complete the task:\n",
    "\n",
    "    1. Extract Relevant Terms:\n",
    "    - Identify key entities and properties explicitly or implicitly mentioned in the question.\n",
    "    - Entities typically include datasets, models, or concepts (e.g., \"ACL Anthology dataset\", \"models\").\n",
    "    - Properties typically describe relationships or actions (e.g., \"evaluated on\").\n",
    "\n",
    "    2. Compute Similarity:\n",
    "    - Retrieve the top 10 most similar entities or properties from the RDF data dump by calculating cosine similarity scores based on their textual representations.\n",
    "\n",
    "    3. Select Top Candidates:\n",
    "    - For each extracted entity or property, return the 5 most relevant matches with the highest similarity scores.\n",
    "\n",
    "    ### Example Question:\n",
    "    The input natural language question is: \"What models are being evaluated on the ACL Anthology dataset?\"\n",
    "\n",
    "    ### Expected JSON Output Format\n",
    "    Return a JSON object where results are grouped by the extracted terms. The structure should be:\n",
    "\n",
    "    {\n",
    "    \"question\": \"What models are being evaluated on the ACL Anthology dataset?\",\n",
    "    \"extracted_terms\": [\n",
    "        {\n",
    "        \"rdfs:label\": \"ACL Anthology dataset\",\n",
    "        \"uri\": \"orkgr:ACL_Anthology_dataset\",\n",
    "        \"type\": \"Dataset\",\n",
    "        \"rdf:type\": \"orkgc:Resource\"\n",
    "        },\n",
    "        {\n",
    "        \"rdfs:label\": \"models\",\n",
    "        \"type\": \"Class\",\n",
    "        \"rdf:type\": \"rdfs:Class\"\n",
    "        },\n",
    "        {\n",
    "        \"rdfs:label\": \"evaluated on\",\n",
    "        \"type\": \"Property\",\n",
    "        \"rdf:type\": \"orkgp:Predicate\"\n",
    "        }\n",
    "    ],\n",
    "    \"candidates\": {\n",
    "        \"ACL Anthology dataset\": [\n",
    "        {\n",
    "            \"uri\": \"orkgr:ACL_Dataset\",\n",
    "            \"rdfs:label\": \"ACL Anthology Corpus\",\n",
    "            \"score\": 0.92,\n",
    "            \"type\": \"Dataset\",\n",
    "            \"rdf:type\": \"orkgc:Resource\"\n",
    "        },\n",
    "        {\n",
    "            \"uri\": \"orkgr:NLP_Benchmark\",\n",
    "            \"rdfs:label\": \"NLP Benchmark Dataset\",\n",
    "            \"score\": 0.89,\n",
    "            \"type\": \"Dataset\",\n",
    "            \"rdf:type\": \"orkgc:Resource\"\n",
    "        }\n",
    "        ],\n",
    "        \"models\": [\n",
    "        {\n",
    "            \"uri\": \"orkgc:BERT\",\n",
    "            \"rdfs:label\": \"BERT\",\n",
    "            \"score\": 0.95,\n",
    "            \"type\": \"Model\",\n",
    "            \"rdf:type\": \"rdfs:Class\"\n",
    "        },\n",
    "        {\n",
    "            \"uri\": \"orkgc:GPT\",\n",
    "            \"rdfs:label\": \"GPT\",\n",
    "            \"score\": 0.91,\n",
    "            \"type\": \"Model\",\n",
    "            \"rdf:type\": \"rdfs:Class\"\n",
    "        }\n",
    "        ],\n",
    "        \"evaluated on\": [\n",
    "        {\n",
    "            \"uri\": \"orkgp:Evaluation_Method\",\n",
    "            \"rdfs:label\": \"Evaluation Method\",\n",
    "            \"score\": 0.90,\n",
    "            \"type\": \"Property\",\n",
    "            \"rdf:type\": \"orkgp:Predicate\"\n",
    "        },\n",
    "        {\n",
    "            \"uri\": \"orkgp:Performance_Assessment\",\n",
    "            \"rdfs:label\": \"Performance Assessment\",\n",
    "            \"score\": 0.87,\n",
    "            \"type\": \"Property\",\n",
    "            \"rdf:type\": \"orkgp:Predicate\"\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    }\n",
    "\n",
    "    ### Additional Refinements:\n",
    "    - **Use RDF Prefixes for URIs**:\n",
    "    - `\"orkgp:\"` for properties (predicates).\n",
    "    - `\"orkgc:\"` for classes (concepts).\n",
    "    - `\"orkgr:\"` for specific named resources (datasets, papers, models, etc.).\n",
    "    - `\"rdf:\"` for general RDF terms.\n",
    "    - `\"rdfs:\"` for schema-related terms.\n",
    "\n",
    "    - **Replace 'label' with `rdfs:label`**:\n",
    "    - Ensure the label of each entity or property is stored under `\"rdfs:label\"`.\n",
    "\n",
    "    - **Include Entity Types (`rdf:type`)**:\n",
    "    - `\"rdfs:Class\"` for general categories.\n",
    "    - `\"orkgp:Predicate\"` for relationships/properties.\n",
    "    - `\"orkgc:Resource\"` for dataset, models, or research objects.\n",
    "\n",
    "    - Ensure:\n",
    "    - Extracted terms are accurate representations of key entities and properties in the question.\n",
    "    - The similarity scores reflect the degree of relevance.\n",
    "    - The results are structured correctly for further processing in SPARQL query generation.\n",
    "\n",
    "    The input natural language question is: \"{question}\"\n",
    "    \"\"\"\n",
    "    # print(f\"User Query: , {prompt}\")\n",
    "    print(\"Getting response from LLM...\\n\")\n",
    "\n",
    "    try:\n",
    "        response = get_response_from_llm(query_engine, prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # sleep for 15 minutes\n",
    "        # print(\"Sleeping for 15 minutes because of rate limits...\")\n",
    "        # time.sleep(900)\n",
    "        time.sleep(10)\n",
    "        response = get_response_from_llm(query_engine, prompt)\n",
    "    # print(f\"Response for question {question_id} is: {response}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    # save the response to a txt file in the output directory\n",
    "    with open(f\"{output_dir}/{question_id}.txt\", \"w\") as f:\n",
    "        f.write(str(response))\n",
    "    print(f\"Response for question {question_id} is saved successfully!\")\n",
    "    print(\"------------------------------------------------------------\\n\")\n",
    "print(\"All questions are processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
