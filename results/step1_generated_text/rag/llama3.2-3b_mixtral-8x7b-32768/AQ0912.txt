Question:
What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?

Generated SPARQL:
 ```sparql
    PREFIX orkgp: <http://example.org/orkgp/>
    PREFIX orkgr: <http://example.org/orkgr/>
    PREFIX orkgc: <http://example.org/orkgc/>

    SELECT?evaluationMetrics?benchmarkingModel?dataset
    WHERE {
     ?dataset orkgc:hasResource?resource.
     ?resource orkgc:label "MLDoc".
     ?dataset orkgc:label "Zero-Shot".
     ?dataset orkgc:label "English-to-Italian".
     ?benchmarkingModel orkgp:hasModel?model.
     ?model orkgp:hasEvaluation?evaluation.
     ?evaluation orkgp:hasEvaluationMetric?evaluationMetrics.
     ?benchmarkingModel orkgp:hasBenchmark?benchmark.
     ?benchmark orkgp:hasPerformance?performance.
      FILTER (STR(?performance) = "Common")
    }
    ```
   
