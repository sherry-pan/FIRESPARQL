Question:
What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?

Generated SPARQL:
 PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
     PREFIX owl: <http://www.w3.org/2002/07/owl#>
     PREFIX schema: <http://schema.org/>
     PREFIX orkg: <http://example.org/orkg/>
     PREFIX dbo: <http://dbpedia.org/ontology/>

     SELECT?metric
     WHERE {
      ?metric rdfs:label?metricLabel.
       FILTER (STR(?metricLabel) = "accuracy")
       UNION
       FILTER (STR(?metricLabel) = "f1-score")
       UNION
       FILTER (STR(?metricLabel) = "precision")
       UNION
       FILTER (STR(?metricLabel) = "recall")
       UNION
       FILTER (STR(?metricLabel) = "f2")
       UNION
       FILTER (STR(?metricLabel) = "roc-auc")
     }
     LIMIT 10<|eot_id|>
