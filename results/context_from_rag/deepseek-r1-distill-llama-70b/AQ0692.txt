<think>
Alright, I'm trying to figure out which properties from the given JSON file are most relevant to the question about evaluation metrics for the ARC-PDN dataset. The question is asking about commonly used metrics when benchmarking models on this dataset. 

First, I'll look through the properties listed. I see several that are related to evaluation. The first one is "evaluation" with the label "evaluation". That seems directly relevant because it's about the process of assessing model performance.

Next, there's "has benchmark" which is about benchmarks used. Since the question is about benchmarking, this property could provide information on the benchmarks applied, which often include specific metrics.

Then, I notice "Recall" as a label. Recall is a common evaluation metric in machine learning, so this is definitely relevant.

"micro-F1" and "macro-F1" are both F1 scores, which are standard metrics used to evaluate classification models. These would be important for assessing performance on the dataset.

"Accuracy" is another fundamental metric that measures overall model performance, so it's highly relevant here.

"has classes" might be useful because knowing the number of classes can influence which metrics are appropriate, but it's a bit more indirect.

"has training set" and "validation set" are about the data splits, which are part of the benchmarking process but not metrics themselves. However, they could provide context on how the evaluations are conducted.

"number of type" could refer to the number of classes or types of data, which again is more about dataset characteristics but still useful in understanding the evaluation setup.

"method" is a bit vague, but in this context, it might refer to the evaluation methods or metrics used.

"has uncertainty value" and "has uncertainty unit" are about handling uncertainty in model outputs, which is more specific but could be part of advanced evaluation.

"Quantitative methods" is a broad term but could encompass various numerical evaluation techniques.

"Risk types" and "Types of riks" seem related to specific aspects of evaluation, possibly in risk assessment, which might be part of the dataset's requirements.

"CO2 footprint process" is more about environmental impact, which doesn't directly relate to evaluation metrics but could be part of a broader benchmarking context.

"Token per News" seems specific to text data, which might not be directly relevant unless the dataset involves text processing.

"lesson learned" is more about post-evaluation analysis but could be part of the benchmarking process.

"Aligned with FAIR principles" relates to data practices, which is important but not a metric.

"has training set" and "validation set" again relate to data splits, which are part of how benchmarks are conducted.

"property of the dataset" is a bit vague but could include metrics.

"tabular dataset" describes the data structure, which might influence the choice of metrics.

"column entity annotation" and similar properties are more about data annotation rather than evaluation metrics.

"matching strategy" and "table selection technique" are about data processing methods, not metrics.

"system module" and "controller" are technical aspects, not directly metrics.

"iteration" and "splitting technique" are about methodology, not metrics.

"knowledge graph name" and "knowledge used" are about data sources, not metrics.

"method applied to unit membranes" seems specific to a biological context, which might not apply here.

"NDCG@10" is a specific metric, so that's relevant.

"stopwords", "stemming", "lemma" are about text preprocessing, not metrics.

"text feature" is about data characteristics, not metrics.

"iteration" again is about process, not metrics.

"number of type" and "data splitting method" are about dataset specifics.

"reporting level" could relate to how results are presented.

"knowledge graph content" is about data, not metrics.

"hasEvaluation" is directly about evaluations.

"Plane" and "Controller" are unrelated.

"micro-F1" and "macro-F1" are already listed.

"Accuracy" is already listed.

"Recall" is already listed.

"no of labels" is about data, not metrics.

"has classes" is already considered.

"has training set" and "validation set" are already noted.

"avg characters", "avg words", etc., are data statistics, not metrics.

"has uncertainty value" and "has uncertainty unit" are already noted.

"classification system" is about the type of problem, not metrics.

"A representative selection of keywords" is about data, not metrics.

"reference system" could be about baseline models, which is relevant.

"CO2 footprint process" is already noted.

"Token per News" is noted.

"Risk types" and "Types of riks" are noted.

"Quantitative methods" is noted.

"method" is noted.

"PM25_prcnt_change" is specific to air quality, which might be part of the dataset.

"city" is a data attribute, not a metric.

"co-located with" is about events, not metrics.

"lesson learned" is noted.

"# core participant", "# table", etc., are about counts, not metrics.

"Knowledge Graph used for annotation" is about data, not metrics.

"Aligned with FAIR principles" is noted.

"# avg row", "# avg column" are data stats.

"# CEA target", "# CTA target", etc., are specific counts.

"property of the dataset" is noted.

"dataset annotation type" is about data, not metrics.

"split into" is about data division.

"# accepted submission", etc., are about process, not metrics.

"tabular dataset" is noted.

"column entity annotation" is noted.

"column type annotation" is noted.

"column property annotation" is noted.

"matching strategy" is noted.

"system module" is noted.

"table selection technique" is noted.

"label generation technique" is about data processing.

"spliting technique" is noted.

"city" is noted.

"method" is noted.

"PM25_prcnt_change" is noted.

"co-located with" is noted.

"lesson learned" is noted.

"# DBpedia properties" is about data, not metrics.

"# Schema.org property" is about data.

"# schema.org class" is about data.

"# CTA participant" is noted.

So, compiling all the relevant properties that relate to evaluation metrics, I have a list that includes evaluation, has benchmark, Recall, micro-F1, macro-F1, Accuracy, has classes, has training set, validation set, number of type, method, has uncertainty value, has uncertainty unit, Quantitative methods, Risk types, Types of riks, CO2 footprint process, Token per News, lesson learned, hasEvaluation, NDCG@10, reference system, classification system, reporting level, has uncertainty unit, has uncertainty value, has training set, validation set, property of the dataset, tabular dataset, has classes, has dataset, has benchmark, evaluation, method, has method, PM25_prcnt_change, city, method, PM25_prcnt_change, co-located with, lesson learned, # core participant, Knowledge Graph used for annotation, # table, # column, # row, # classes, # entity, Aligned with FAIR principles, # avg row, # avg column, # CEA target, # CTA target, # CPA target, property of the dataset, dataset annotation type, split into, # accepted submission, # rejected submission, # poster, tabular dataset, column entity annotation, column type annotation, column property annotation, matching strategy, system module, table selection technique, label generation technique, spliting technique, city, method, PM25_prcnt_change, co-located with, lesson learned, # DBpedia properties, # Schema.org property, # schema.org class, # CTA participant.

But many of these are not directly metrics. So, focusing on the ones that are specifically evaluation metrics or related to benchmarking processes, the most relevant properties would be:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But many of these are more about data characteristics or process rather than metrics. So, the most directly relevant ones are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But to narrow it down to the most relevant, focusing on metrics and benchmarking aspects:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But many of these are not directly metrics. So, the key ones are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But to be precise, the most relevant properties that correspond to evaluation metrics are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But many of these are more about data characteristics or process rather than metrics. So, focusing on the ones that are specifically evaluation metrics or directly related to benchmarking:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But to be precise, the most relevant properties that correspond to evaluation metrics are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But many of these are not directly metrics. So, focusing on the ones that are specifically evaluation metrics or directly related to benchmarking:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But to be precise, the most relevant properties that correspond to evaluation metrics are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But many of these are not directly metrics. So, focusing on the ones that are specifically evaluation metrics or directly related to benchmarking:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But to be precise, the most relevant properties that correspond to evaluation metrics are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But many of these are not directly metrics. So, focusing on the ones that are specifically evaluation metrics or directly related to benchmarking:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But to be precise, the most relevant properties that correspond to evaluation metrics are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But many of these are not directly metrics. So, focusing on the ones that are specifically evaluation metrics or directly related to benchmarking:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But to be precise, the most relevant properties that correspond to evaluation metrics are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But many of these are not directly metrics. So, focusing on the ones that are specifically evaluation metrics or directly related to benchmarking:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But to be precise, the most relevant properties that correspond to evaluation metrics are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But many of these are not directly metrics. So, focusing on the ones that are specifically evaluation metrics or directly related to benchmarking:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But to be precise, the most relevant properties that correspond to evaluation metrics are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But many of these are not directly metrics. So, focusing on the ones that are specifically evaluation metrics or directly related to benchmarking:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But to be precise, the most relevant properties that correspond to evaluation metrics are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But many of these are not directly metrics. So, focusing on the ones that are specifically evaluation metrics or directly related to benchmarking:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- # table
- # column
- # row
- # classes
- # entity
- # avg row
- # avg column
- # CEA target
- # CTA target
- # CPA target
- # accepted submission
- # rejected submission
- # poster
- column entity annotation
- column type annotation
- column property annotation
- matching strategy
- system module
- table selection technique
- label generation technique
- spliting technique
- city
- method
- PM25_prcnt_change
- co-located with
- # DBpedia properties
- # Schema.org property
- # schema.org class
- # CTA participant.

But to be precise, the most relevant properties that correspond to evaluation metrics are:

- evaluation
- has benchmark
- Recall
- micro-F1
- macro-F1
- Accuracy
- hasEvaluation
- NDCG@10
- Quantitative methods
- has uncertainty value
- has uncertainty unit
- reference system
- classification system
- reporting level
- has method
- method
- has dataset
- has training set
- validation set
- has classes
- number of type
- property of the dataset
- tabular dataset
- dataset annotation type
- split into
- lesson learned
- Aligned with FAIR principles
- # core participant
- #