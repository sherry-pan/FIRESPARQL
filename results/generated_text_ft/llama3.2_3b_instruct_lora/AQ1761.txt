Question:
Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?

Generated SPARQL:
 SELECT DISTINCT?model?model_lbl
      WHERE {
       ?metric      a       orkgc:Metric;
                    rdfs:label ?metric_lbl.
        FILTER (str(?metric_lbl) = "Accuracy")
       ?benchmark      orkgp:HAS_DATASET       ?dataset.
       ?dataset       orkgp:HAS_BENCHMARK     ?benchmark.
       ?benchmark      orkgp:HAS_MODEL         ?model.
       ?model        rdfs:label              ?model_lbl.
       ?model      orkgp:P32               ?cont.
       ?cont       orkgp:HAS_EVALUATION   ?eval.
       ?eval        orkgp:HAS_METRIC        ?metric;
                    orkgp:P31               ?eval_metric.
       ?eval_metric    rdfs:label              ?eval_metric_lbl.
       ?cont      orkgp:HAS_MODEL         ?model.
       ?cont      orkgp:P32               ?cont.
      }    .      LIMIT 1;      OFFSET 0;      ORDER BY DESC(?eval_metric_lbl).      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;      END;     
