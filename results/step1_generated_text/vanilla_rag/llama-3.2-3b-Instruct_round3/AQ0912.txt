Question:
What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?

Generated SPARQL:
 PREFIX orkg: <http://orkg.org/orkg/predicate/>
     SELECT?metric 
     WHERE {
    ?benchmark a orkg:Benchmark.
    ?benchmark orkg:has_evaluation?metric.
     FILTER (lang(?metric) = "en" AND?metric IN ("accuracy", "f1-score", "roc-auc"))
     }
    <|eot_id|>
