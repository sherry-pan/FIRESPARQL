Question:
What are the metrics of evaluation over the Atari 2600 Defender dataset?

Generated SPARQL:
 PREFIX orkg: <http://www.openresearchknowledgegraph.org/ontology/orkg#>
     PREFIX dbo: <http://dbpedia.org/ontology/>
     PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
     PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
     SELECT?metric?dataset 
     WHERE {
        ?metric orkg:isEvaluationMetricOf?dataset.
        ?dataset dbo:datasetName "Atari 2600 Defender".
     } GROUP BY?metric?dataset 
     HAVING (COUNT(?metric) > 1 OR COUNT(?dataset) > 1) 
     ORDER BY DESC(?metric) 
     LIMIT 10
    Note: The above SPARQL query is just an example, the actual query may need to be adjusted according to the actual data in the ORKG. 
    Example Use Cases: 
    - To get the metrics of evaluation over the MNIST dataset, you can modify the dataset name in the query to "MNIST" and run the query. 
    - To get the metrics of evaluation over the IMDB dataset, you can modify the dataset name in the query to "IMDB" and run the query. 
    - To get the metrics of evaluation over the CIFAR-10 dataset, you can modify the dataset name in the query to "CIFAR-10" and run the query. 
    - To get the metrics of evaluation over the ImageNet dataset, you can modify the dataset name in the query to "ImageNet" and run the query. 
    - To get the metrics of evaluation over the Reuters-08 dataset, you can modify the dataset name in the query to "Reuters-08" and run the query. 
    - To get the metrics of evaluation over the 20 Newsgroups dataset, you can modify the dataset name in the query to "20 Newsgroups" and run the query. 
    - To get the metrics of evaluation over the Stanford Question Answering Dataset (SQuAD) dataset, you can modify the dataset name in the query to "SQuAD" and run the query. 
    - To get the metrics of evaluation over the Question Answering Dataset 2 (QuAD) dataset, you can modify the dataset name in the ORKG to "Question Answering Dataset 2" and run the query. 
    - To get the metrics of evaluation over the GLUE dataset, you can modify the dataset name in the query to "GLUE" and run the query. 
    - To get the metrics of evaluation over the SuperGLUE dataset, you can modify the dataset name in the query to "SuperGLUE" and run the query. 
    - To get the metrics of evaluation over the MultiNLI dataset, you can modify the dataset name in the query to "MultiNLI" and run the query. 
    - To get the metrics of evaluation over the Stanford Sentiment Treebank dataset, you can modify the dataset name in the query to "Stanford Sentiment Treebank" and run the query. 
    - To get the metrics of evaluation over the SICK-E dataset, you can modify the dataset name in the query to "SICK-E" and run the query. 
    - To get the metrics of evaluation over the SICK-R dataset, you can modify the dataset name in the query to "SICK-R" and run the query. 
    - To get the metrics of evaluation over the Multi-Genre Movie Review Dataset, you can modify the dataset name in the query to "Multi-Genre Movie Review Dataset" and run the query. 
    - To get the metrics of evaluation over the Movie Reviews Dataset, you can modify the dataset name in the query to "Movie Reviews Dataset" and run the query. 
    - To get the metrics of evaluation over the 9 Sentiment Analysis in Healthcare Dataset, you can modify the dataset name in the query to "9 Sentiment Analysis in Healthcare Dataset" and run the query. 
    - To get the metrics of evaluation over the Multi-Task Sentiment Analysis Dataset, you can modify the dataset name in the query to "Multi-Task Sentiment Analysis Dataset" and run the query. 
    - To get the metrics of evaluation over the Aspect-Based Sent
