Question:
What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?

Generated SPARQL:
 PREFIX orkg: <http://openresearchknowledge.org/ontology/>
PREFIX dc: <http://purl.org/dc/elements/1.1/>
PREFIX dcterms: <http://purl.org/dc/terms/>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

SELECT?metric
WHERE {
 ?paper a orkg:ResearchPaper;
    dcterms:title?title;
    orkg:hasDataset?dataset;
    orkg:hasEvaluation?evaluation.
 ?dataset a orkg:Dataset;
    dc:title "Habitat 2020 Object Nav test-std".
 ?evaluation a orkg:Evaluation;
    orkg:hasMetric?metric.
  FILTER (?title = "Benchmarking models on the Habitat 2020 Object Nav test-std dataset" ||
         regex(str(?title), "Benchmarking models on the Habitat 2020 Object Nav test-std dataset", "i")).
}<|eot_id|>
